{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fead19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3c98225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma, Komoran, Hannanum, Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21ad6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3285089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ff3d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f6af53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce7ac695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_writer</th>\n",
       "      <th>date_writer</th>\n",
       "      <th>title</th>\n",
       "      <th>book_writer</th>\n",
       "      <th>book_company</th>\n",
       "      <th>book_date</th>\n",
       "      <th>report_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>박유담</td>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>군주론</td>\n",
       "      <td>마키아벨리</td>\n",
       "      <td>현대지성</td>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>마키아벨리의 군주론을 읽고21세기를 살아가고 있는 우리에게 리더의 역할은 더욱...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>백석</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>빛 속으로 (한국 문학사에서 지워진 이름. 평생을 방랑자로 산 작가 김사량의 작품집)</td>\n",
       "      <td>김사량</td>\n",
       "      <td>녹색광선</td>\n",
       "      <td>2021-08-15</td>\n",
       "      <td>&lt;비뚤어진 우월감&gt;  우리는 때때로 자신의 치부로 울타리를 두르고 그 안에 갇...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maobi</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>작별하지 않는다 (한강 장편소설)</td>\n",
       "      <td>한강</td>\n",
       "      <td>문학동네</td>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>다시 한강 작가님의 책을 읽을 때까지는 고민이 있었다. '소년이 온다'를 읽...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>킹고선비</td>\n",
       "      <td>2022-04-10</td>\n",
       "      <td>나의 한국현대사 1959-2020</td>\n",
       "      <td>유시민</td>\n",
       "      <td>돌베개</td>\n",
       "      <td>2021-01-20</td>\n",
       "      <td>59년생 ‘68세대’의 회고록을 읽은 97년생 ‘MZ세대’의 감상문    몇 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nama</td>\n",
       "      <td>2022-04-08</td>\n",
       "      <td>그냥 하지 말라 (당신의 모든 것이 메시지다)</td>\n",
       "      <td>송길영</td>\n",
       "      <td>북스톤</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>저자는 과거부터 현재까지 변화해온, 또 앞으로 변화할 한국사회를 데이터를 통해...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  report_writer date_writer                                            title  \\\n",
       "0           박유담  2022-05-02                                              군주론   \n",
       "1            백석  2022-04-17  빛 속으로 (한국 문학사에서 지워진 이름. 평생을 방랑자로 산 작가 김사량의 작품집)   \n",
       "2         maobi  2022-04-16                               작별하지 않는다 (한강 장편소설)   \n",
       "3          킹고선비  2022-04-10                               나의 한국현대사 1959-2020   \n",
       "4          nama  2022-04-08                        그냥 하지 말라 (당신의 모든 것이 메시지다)   \n",
       "\n",
       "  book_writer book_company   book_date  \\\n",
       "0       마키아벨리         현대지성  2021-07-26   \n",
       "1         김사량         녹색광선  2021-08-15   \n",
       "2          한강         문학동네  2021-09-09   \n",
       "3         유시민          돌베개  2021-01-20   \n",
       "4         송길영          북스톤  2021-10-05   \n",
       "\n",
       "                                         report_text  \n",
       "0     마키아벨리의 군주론을 읽고21세기를 살아가고 있는 우리에게 리더의 역할은 더욱...  \n",
       "1     <비뚤어진 우월감>  우리는 때때로 자신의 치부로 울타리를 두르고 그 안에 갇...  \n",
       "2      다시 한강 작가님의 책을 읽을 때까지는 고민이 있었다. '소년이 온다'를 읽...  \n",
       "3     59년생 ‘68세대’의 회고록을 읽은 97년생 ‘MZ세대’의 감상문    몇 ...  \n",
       "4     저자는 과거부터 현재까지 변화해온, 또 앞으로 변화할 한국사회를 데이터를 통해...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./fivecart_2020.csv\", encoding='cp949')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32ad2100",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   저자는 과거부터 현재까지 변화해온, 또 앞으로 변화할 한국사회를 데이터를 통해 들여다보고 이야기한다. 비슷한 결의 책을 여러 권 읽어봤지만 단순히 현상만 나열하는 게 아니라 저자의 통찰이 깊게 느껴진 책은 정말 오랜만이었다. 그러면서도 어렵지 않게 쓴다는 것이 대단하다고 생각했다. 세상에 똑똑한 사람은 정말 많지만 어려운 이야기를 이해하기 쉽게 전달하는 능력을 가진 사람은 그리 많지 않은 것 같다. 그래서 더 반갑고 좋았다.  이 책은 변화하는 사회를 살아가기 위해 사회구성원 개개인이 취해야 할 자세를 제시한다. 젊은 세대가 읽어도 좋지만 특히 관리자층이 이 책을 읽어봤으면 하는 바람이다. 중간관리자들의 무능과 고집을 날카롭게 지적하는 부분이 많았다. 나는 아직 사회생활을 제대로 시작해보지 않았지만 가족, 친구들과 대화하다보면 꼭 일도 못하면서 남에게 책임만 전가하는 상사 얘기가 나온다. 나는 이런 어른이 되지는 말아야지. 스스로를 경계하기 위해서도 한 번쯤 읽어보면 좋을 책이다. 중간중간 뜨끔하는 문장이 있을 수도 있다.[당신의 모든 것이 메시지다]책의 부제이기도 한 이 챕터가 가장 인상적이었다. 요즘 들어 기록의 중요성을 점점 체감하고 있다. 개인적으로 기억하기 위함도 있지만 어딘가에 내가 한 활동들과 내 생각을 보여줘야할 때 특히 더 절실히 느낀다. 지금까지는 그걸 기록해야 한다는 생각을 못했는데 이제 와서 정리하려고 보니 내가 했던 게 뭔지, 그걸 통해 뭘 배우고 느꼈는지 잘 기억이 안 나서 막막했다. 그제서야 어떤 형태로든 기록을 해야겠구나 싶었다. 이제 더는 학벌이나 성적이 나를 증명해주지 않는다. 요즘 들어 그걸 정말 많이 느끼고 있고, 앞으로는 세상이 더욱 그런 방향으로 변해갈 것이라 생각한다.저자는 일상의 모든 기록들이 자신을 표현하는 메시지가 된다고 말한다. 이력서나 포트폴리오로 자신을 소개하는 것을 넘어 이제는 소비로도, sns관계망으도 나를 표현하는 시대가 온 것이다. 사람들은 이제 필요에 의해 구매하는 물질소비가 아닌, 자신의 개성을 드러내고 의미를 표현하기 위한 의미소비를 한다. 나 또한 요즘 들어 소비 패턴이 이런 식으로 변하고 있다. 같은 물건이더라도 브랜드가 추구하는 가치가 내가 지향하는 바와 맞는 경우에는 더 즐거운 마음으로 소비를 하게 되고 해당 브랜드를 기억하게 되는 것 같다.콘텐츠 소비 또한 마찬가지로 내가 어떤 콘텐츠를 선택하느냐는 곧 나의 취향을 은연중에 알리는 것과도 같다. 나의 sns에서 누구를 팔로우하느냐도 나의 관심사와 네트워크를 공개하는 행위의 일종인 셈이다. 저자는 결국 이 모든 것이 수렴된 나의 라이프스타일, 나의 모든 것이 메시지가 된다고 말하고 있다.한편으로는 나의 모든 기록이 나를 보여주는 수단이 된다는 게 조금 숨막히기도 하고 무섭게 느껴지기도 한다. 비밀이 없는 사회가 되어가고 있는 것 같다. 나는 지금까지 현실에 안주하며 변화를 두려워하고 거부해왔다. 어쩌면 나는 그 변화에 해당되지 않을 거라고 생각했던 것 같기도 하다. 그러나 저자가 서문에서 이야기했듯이 아직 나에게 일어나지 않았다 해도 변화는 분명히 일어나고 있다. 언젠가는 나에게도 올 그 변화를 맞이할 준비를 해야할 것이다. 모든 것에 장단점이 있듯이 새로운 세상의 장점도 분명히 있을테니 조금은 덜 무서워해도 되지 않을까?   '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = data['report_text'][4]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "163766df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keybert in c:\\users\\liber\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from keybert) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from keybert) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from keybert) (1.23.4)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from keybert) (13.5.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.15.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (4.7.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (2.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.19.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (1.11.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.12.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (23.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\liber\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.12.1)\n",
      "Requirement already satisfied: click in c:\\users\\liber\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2023.7.22)\n",
      "Requirement already satisfied: kiwipiepy in c:\\users\\liber\\anaconda3\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: kiwipiepy-model~=0.16 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from kiwipiepy) (0.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\liber\\anaconda3\\lib\\site-packages (from kiwipiepy) (1.23.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\liber\\anaconda3\\lib\\site-packages (from kiwipiepy) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\liber\\anaconda3\\lib\\site-packages (from tqdm->kiwipiepy) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install keybert\n",
    "# !pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47134032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\users\\liber\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.19.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.11.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\liber\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
      "Requirement already satisfied: click in c:\\users\\liber\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\liber\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4679131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.object = object    \n",
    "np.int = int\n",
    "np.float = float\n",
    "np.bool = bool\n",
    "# import keyBERT np.object 오류 제거 위해 직접 대입 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8fbb964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89bf4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a7fcb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8914f1c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c6112fbf114075ab93c38bbe485205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0fe39/.gitattributes:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6288994590455d9f6ed73fee44b79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e379009d33b6487e8fa1214c4811fb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)83e900fe39/README.md:   0%|          | 0.00/3.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496d6337da6c4ef2b141e8b795fc801f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e900fe39/config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d712260c49e4e9dbb839fcce74f34a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00eea08d1684633a54e34c7e2bb01f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db65074c91ae4b9b879213e60c3e6507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc17925f5c14a95ba34de6c3300159e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afde1e0671004677825ecac5bbf05051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3754505611d4ef5aecda04264dfc1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d3529a574849e0a71d267ad3d9354b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1f8a786c0a42368f1017a7d02265f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading unigram.json:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2aa1e8e17c04d0db5f0f04602252abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)900fe39/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('책이다', 0.4772),\n",
       " ('책을', 0.441),\n",
       " ('책은', 0.405),\n",
       " ('책의', 0.3337),\n",
       " ('이야기를', 0.3304),\n",
       " ('한국사회를', 0.3293),\n",
       " ('말한다', 0.3106),\n",
       " ('무능과', 0.2832),\n",
       " ('과거부터', 0.2673),\n",
       " ('능력을', 0.256)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b991fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "15751396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram 개수 : 364\n",
      "trigram 다섯개만 출력 : ['sns관계망으도 나를 표현하는' 'sns에서 누구를 팔로우하느냐도' '가장 인상적이었다 요즘' '가족 친구들과 대화하다보면'\n",
      " '가진 사람은 그리']\n"
     ]
    }
   ],
   "source": [
    "# 3개의 단어 묶음인 단어구 추출\n",
    "n_gram_range = (3, 3)\n",
    "stop_words = \"english\"\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 :',len(candidates))\n",
    "print('trigram 다섯개만 출력 :',candidates[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ef9c184b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c0f93cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고집을 날카롭게 지적하는', '날카롭게 지적하는 부분이', '친구들과 대화하다보면 일도', '안주하며 변화를 두려워하고', '가족 친구들과 대화하다보면']\n"
     ]
    }
   ],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47df1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    # 문서와 각 키워드들 간의 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4db6e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eb733889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['의해 구매하는 물질소비가',\n",
       " '관심사와 네트워크를 공개하는',\n",
       " '고집을 날카롭게 지적하는',\n",
       " '친구들과 대화하다보면 일도',\n",
       " '가족 친구들과 대화하다보면']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0d91296e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['언젠가는 나에게도 변화를',\n",
       " '현실에 안주하며 변화를',\n",
       " '포트폴리오로 자신을 소개하는',\n",
       " '바람이다 중간관리자들의 무능과',\n",
       " '친구들과 대화하다보면 일도']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bbc15f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가족 친구들과 대화하다보면',\n",
       " '경계하기 위해서도 번쯤',\n",
       " '스스로를 경계하기 위해서도',\n",
       " '얘기가 나온다 나는',\n",
       " '패턴이 이런 식으로']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad197ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4ecda1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([Token(form='저자', tag='NNG', start=3, len=2),\n",
       "   Token(form='는', tag='JX', start=5, len=1),\n",
       "   Token(form='과거', tag='NNG', start=7, len=2),\n",
       "   Token(form='부터', tag='JX', start=9, len=2),\n",
       "   Token(form='현재', tag='NNG', start=12, len=2),\n",
       "   Token(form='까지', tag='JX', start=14, len=2),\n",
       "   Token(form='변화', tag='NNG', start=17, len=2),\n",
       "   Token(form='하', tag='XSV', start=19, len=1),\n",
       "   Token(form='어', tag='EC', start=19, len=1),\n",
       "   Token(form='오', tag='VX', start=20, len=1),\n",
       "   Token(form='ᆫ', tag='ETM', start=20, len=1),\n",
       "   Token(form=',', tag='SP', start=21, len=1),\n",
       "   Token(form='또', tag='MAG', start=23, len=1),\n",
       "   Token(form='앞', tag='NNG', start=25, len=1),\n",
       "   Token(form='으로', tag='JKB', start=26, len=2),\n",
       "   Token(form='변화', tag='NNG', start=29, len=2),\n",
       "   Token(form='하', tag='XSV', start=31, len=1),\n",
       "   Token(form='ᆯ', tag='ETM', start=31, len=1),\n",
       "   Token(form='한국', tag='NNP', start=33, len=2),\n",
       "   Token(form='사회', tag='NNG', start=35, len=2),\n",
       "   Token(form='를', tag='JKO', start=37, len=1),\n",
       "   Token(form='데이터', tag='NNG', start=39, len=3),\n",
       "   Token(form='를', tag='JKO', start=42, len=1),\n",
       "   Token(form='통하', tag='VV', start=44, len=2),\n",
       "   Token(form='어', tag='EC', start=45, len=1),\n",
       "   Token(form='들여다보', tag='VV', start=47, len=4),\n",
       "   Token(form='고', tag='EC', start=51, len=1),\n",
       "   Token(form='이야기', tag='NNG', start=53, len=3),\n",
       "   Token(form='하', tag='XSV', start=56, len=1),\n",
       "   Token(form='ᆫ다', tag='EF', start=56, len=2),\n",
       "   Token(form='.', tag='SF', start=58, len=1),\n",
       "   Token(form='비슷', tag='XR', start=60, len=2),\n",
       "   Token(form='하', tag='XSA', start=62, len=1),\n",
       "   Token(form='ᆫ', tag='ETM', start=62, len=1),\n",
       "   Token(form='결의', tag='NNG', start=64, len=2),\n",
       "   Token(form='책', tag='NNG', start=67, len=1),\n",
       "   Token(form='을', tag='JKO', start=68, len=1),\n",
       "   Token(form='여러', tag='MM', start=70, len=2),\n",
       "   Token(form='권', tag='NNB', start=73, len=1),\n",
       "   Token(form='읽', tag='VV', start=75, len=1),\n",
       "   Token(form='어', tag='EC', start=76, len=1),\n",
       "   Token(form='보', tag='VX', start=77, len=1),\n",
       "   Token(form='었', tag='EP', start=77, len=1),\n",
       "   Token(form='지만', tag='EC', start=78, len=2),\n",
       "   Token(form='단순히', tag='MAG', start=81, len=3),\n",
       "   Token(form='현상', tag='NNG', start=85, len=2),\n",
       "   Token(form='만', tag='JX', start=87, len=1),\n",
       "   Token(form='나열', tag='NNG', start=89, len=2),\n",
       "   Token(form='하', tag='XSV', start=91, len=1),\n",
       "   Token(form='는', tag='ETM', start=92, len=1),\n",
       "   Token(form='것', tag='NNB', start=94, len=1),\n",
       "   Token(form='이', tag='JKC', start=94, len=1),\n",
       "   Token(form='아니', tag='VCN', start=96, len=2),\n",
       "   Token(form='라', tag='EC', start=98, len=1),\n",
       "   Token(form='저자', tag='NNG', start=100, len=2),\n",
       "   Token(form='의', tag='JKG', start=102, len=1),\n",
       "   Token(form='통찰', tag='NNG', start=104, len=2),\n",
       "   Token(form='이', tag='JKS', start=106, len=1),\n",
       "   Token(form='깊', tag='VA', start=108, len=1),\n",
       "   Token(form='게', tag='EC', start=109, len=1),\n",
       "   Token(form='느끼', tag='VV', start=111, len=2),\n",
       "   Token(form='어', tag='EC', start=112, len=1),\n",
       "   Token(form='지', tag='VX', start=113, len=1),\n",
       "   Token(form='ᆫ', tag='ETM', start=113, len=1),\n",
       "   Token(form='책', tag='NNG', start=115, len=1),\n",
       "   Token(form='은', tag='JX', start=116, len=1),\n",
       "   Token(form='정말', tag='MAG', start=118, len=2),\n",
       "   Token(form='오랜만', tag='NNG', start=121, len=3),\n",
       "   Token(form='이', tag='VCP', start=124, len=1),\n",
       "   Token(form='었', tag='EP', start=125, len=1),\n",
       "   Token(form='다', tag='EF', start=126, len=1),\n",
       "   Token(form='.', tag='SF', start=127, len=1),\n",
       "   Token(form='그러', tag='VV', start=129, len=2),\n",
       "   Token(form='면서', tag='EC', start=131, len=2),\n",
       "   Token(form='도', tag='JX', start=133, len=1),\n",
       "   Token(form='어렵', tag='VA-I', start=135, len=2),\n",
       "   Token(form='지', tag='EC', start=137, len=1),\n",
       "   Token(form='않', tag='VX', start=139, len=1),\n",
       "   Token(form='게', tag='EC', start=140, len=1),\n",
       "   Token(form='쓰', tag='VV', start=142, len=1),\n",
       "   Token(form='ᆫ다는', tag='ETM', start=142, len=3),\n",
       "   Token(form='것', tag='NNB', start=146, len=1),\n",
       "   Token(form='이', tag='JKS', start=147, len=1),\n",
       "   Token(form='대단', tag='XR', start=149, len=2),\n",
       "   Token(form='하', tag='XSA', start=151, len=1),\n",
       "   Token(form='다고', tag='EC', start=152, len=2),\n",
       "   Token(form='생각', tag='NNG', start=155, len=2),\n",
       "   Token(form='하', tag='XSV', start=157, len=1),\n",
       "   Token(form='었', tag='EP', start=157, len=1),\n",
       "   Token(form='다', tag='EF', start=158, len=1),\n",
       "   Token(form='.', tag='SF', start=159, len=1),\n",
       "   Token(form='세상에', tag='IC', start=161, len=3),\n",
       "   Token(form='똑똑', tag='XR', start=165, len=2),\n",
       "   Token(form='하', tag='XSA', start=167, len=1),\n",
       "   Token(form='ᆫ', tag='ETM', start=167, len=1),\n",
       "   Token(form='사람', tag='NNG', start=169, len=2),\n",
       "   Token(form='은', tag='JX', start=171, len=1),\n",
       "   Token(form='정말', tag='MAG', start=173, len=2),\n",
       "   Token(form='많', tag='VA', start=176, len=1),\n",
       "   Token(form='지만', tag='EC', start=177, len=2),\n",
       "   Token(form='어렵', tag='VA-I', start=180, len=3),\n",
       "   Token(form='은', tag='ETM', start=182, len=1),\n",
       "   Token(form='이야기', tag='NNG', start=184, len=3),\n",
       "   Token(form='를', tag='JKO', start=187, len=1),\n",
       "   Token(form='이해', tag='NNG', start=189, len=2),\n",
       "   Token(form='하', tag='XSV', start=191, len=1),\n",
       "   Token(form='기', tag='ETN', start=192, len=1),\n",
       "   Token(form='쉽', tag='VA-I', start=194, len=1),\n",
       "   Token(form='게', tag='EC', start=195, len=1),\n",
       "   Token(form='전달', tag='NNG', start=197, len=2),\n",
       "   Token(form='하', tag='XSV', start=199, len=1),\n",
       "   Token(form='는', tag='ETM', start=200, len=1),\n",
       "   Token(form='능력', tag='NNG', start=202, len=2),\n",
       "   Token(form='을', tag='JKO', start=204, len=1),\n",
       "   Token(form='가지', tag='VV', start=206, len=2),\n",
       "   Token(form='ᆫ', tag='ETM', start=207, len=1),\n",
       "   Token(form='사람', tag='NNG', start=209, len=2),\n",
       "   Token(form='은', tag='JX', start=211, len=1),\n",
       "   Token(form='그리', tag='MAG', start=213, len=2),\n",
       "   Token(form='많', tag='VA', start=216, len=1),\n",
       "   Token(form='지', tag='EC', start=217, len=1),\n",
       "   Token(form='않', tag='VX', start=219, len=1),\n",
       "   Token(form='은', tag='ETM', start=220, len=1),\n",
       "   Token(form='것', tag='NNB', start=222, len=1),\n",
       "   Token(form='같', tag='VA', start=224, len=1),\n",
       "   Token(form='다', tag='EF', start=225, len=1),\n",
       "   Token(form='.', tag='SF', start=226, len=1),\n",
       "   Token(form='그렇', tag='VA', start=228, len=2),\n",
       "   Token(form='어서', tag='EC', start=229, len=2),\n",
       "   Token(form='더', tag='MAG', start=232, len=1),\n",
       "   Token(form='반갑', tag='VA-I', start=234, len=2),\n",
       "   Token(form='고', tag='EC', start=236, len=1),\n",
       "   Token(form='좋', tag='VA', start=238, len=1),\n",
       "   Token(form='었', tag='EP', start=239, len=1),\n",
       "   Token(form='다', tag='EF', start=240, len=1),\n",
       "   Token(form='.', tag='SF', start=241, len=1),\n",
       "   Token(form='이', tag='MM', start=244, len=1),\n",
       "   Token(form='책', tag='NNG', start=246, len=1),\n",
       "   Token(form='은', tag='JX', start=247, len=1),\n",
       "   Token(form='변화', tag='NNG', start=249, len=2),\n",
       "   Token(form='하', tag='XSV', start=251, len=1),\n",
       "   Token(form='는', tag='ETM', start=252, len=1),\n",
       "   Token(form='사회', tag='NNG', start=254, len=2),\n",
       "   Token(form='를', tag='JKO', start=256, len=1),\n",
       "   Token(form='살아가', tag='VV', start=258, len=3),\n",
       "   Token(form='기', tag='ETN', start=261, len=1),\n",
       "   Token(form='위하', tag='VV', start=263, len=2),\n",
       "   Token(form='어', tag='EC', start=264, len=1),\n",
       "   Token(form='사회', tag='NNG', start=266, len=2),\n",
       "   Token(form='구성원', tag='NNG', start=268, len=3),\n",
       "   Token(form='개개인', tag='NNG', start=272, len=3),\n",
       "   Token(form='이', tag='JKS', start=275, len=1),\n",
       "   Token(form='취하', tag='VV', start=277, len=2),\n",
       "   Token(form='어야', tag='EC', start=278, len=2),\n",
       "   Token(form='하', tag='VX', start=281, len=1),\n",
       "   Token(form='ᆯ', tag='ETM', start=281, len=1),\n",
       "   Token(form='자세', tag='NNG', start=283, len=2),\n",
       "   Token(form='를', tag='JKO', start=285, len=1),\n",
       "   Token(form='제시', tag='NNG', start=287, len=2),\n",
       "   Token(form='하', tag='XSV', start=289, len=1),\n",
       "   Token(form='ᆫ다', tag='EF', start=289, len=2),\n",
       "   Token(form='.', tag='SF', start=291, len=1),\n",
       "   Token(form='젊', tag='VA', start=293, len=1),\n",
       "   Token(form='은', tag='ETM', start=294, len=1),\n",
       "   Token(form='세대', tag='NNG', start=296, len=2),\n",
       "   Token(form='가', tag='JKS', start=298, len=1),\n",
       "   Token(form='읽', tag='VV', start=300, len=1),\n",
       "   Token(form='어도', tag='EC', start=301, len=2),\n",
       "   Token(form='좋', tag='VA', start=304, len=1),\n",
       "   Token(form='지만', tag='EC', start=305, len=2),\n",
       "   Token(form='특히', tag='MAG', start=308, len=2),\n",
       "   Token(form='관리자', tag='NNG', start=311, len=3),\n",
       "   Token(form='층', tag='XSN', start=314, len=1),\n",
       "   Token(form='이', tag='JKS', start=315, len=1),\n",
       "   Token(form='이', tag='MM', start=317, len=1),\n",
       "   Token(form='책', tag='NNG', start=319, len=1),\n",
       "   Token(form='을', tag='JKO', start=320, len=1),\n",
       "   Token(form='읽', tag='VV', start=322, len=1),\n",
       "   Token(form='어', tag='EC', start=323, len=1),\n",
       "   Token(form='보', tag='VX', start=324, len=1),\n",
       "   Token(form='었', tag='EP', start=324, len=1),\n",
       "   Token(form='으면', tag='EC', start=325, len=2),\n",
       "   Token(form='하', tag='VX', start=328, len=1),\n",
       "   Token(form='는', tag='ETM', start=329, len=1),\n",
       "   Token(form='바람', tag='NNG', start=331, len=2),\n",
       "   Token(form='이', tag='VCP', start=333, len=1),\n",
       "   Token(form='다', tag='EF', start=334, len=1),\n",
       "   Token(form='.', tag='SF', start=335, len=1),\n",
       "   Token(form='중간', tag='NNG', start=337, len=2),\n",
       "   Token(form='관리자', tag='NNG', start=339, len=3),\n",
       "   Token(form='들', tag='XSN', start=342, len=1),\n",
       "   Token(form='의', tag='JKG', start=343, len=1),\n",
       "   Token(form='무능', tag='NNG', start=345, len=2),\n",
       "   Token(form='과', tag='JC', start=347, len=1),\n",
       "   Token(form='고집', tag='NNG', start=349, len=2),\n",
       "   Token(form='을', tag='JKO', start=351, len=1),\n",
       "   Token(form='날카롭', tag='VA-I', start=353, len=3),\n",
       "   Token(form='게', tag='EC', start=356, len=1),\n",
       "   Token(form='지적', tag='NNG', start=358, len=2),\n",
       "   Token(form='하', tag='XSV', start=360, len=1),\n",
       "   Token(form='는', tag='ETM', start=361, len=1),\n",
       "   Token(form='부분', tag='NNG', start=363, len=2),\n",
       "   Token(form='이', tag='JKS', start=365, len=1),\n",
       "   Token(form='많', tag='VA', start=367, len=1),\n",
       "   Token(form='었', tag='EP', start=368, len=1),\n",
       "   Token(form='다', tag='EF', start=369, len=1),\n",
       "   Token(form='.', tag='SF', start=370, len=1),\n",
       "   Token(form='나', tag='NP', start=372, len=1),\n",
       "   Token(form='는', tag='JX', start=373, len=1),\n",
       "   Token(form='아직', tag='MAG', start=375, len=2),\n",
       "   Token(form='사회생활', tag='NNG', start=378, len=4),\n",
       "   Token(form='을', tag='JKO', start=382, len=1),\n",
       "   Token(form='제대로', tag='MAG', start=384, len=3),\n",
       "   Token(form='시작', tag='NNG', start=388, len=2),\n",
       "   Token(form='하', tag='XSV', start=390, len=1),\n",
       "   Token(form='어', tag='EC', start=390, len=1),\n",
       "   Token(form='보', tag='VX', start=391, len=1),\n",
       "   Token(form='지', tag='EC', start=392, len=1),\n",
       "   Token(form='않', tag='VX', start=394, len=1),\n",
       "   Token(form='었', tag='EP', start=395, len=1),\n",
       "   Token(form='지만', tag='EC', start=396, len=2),\n",
       "   Token(form='가족', tag='NNG', start=399, len=2),\n",
       "   Token(form=',', tag='SP', start=401, len=1),\n",
       "   Token(form='친구', tag='NNG', start=403, len=2),\n",
       "   Token(form='들', tag='XSN', start=405, len=1),\n",
       "   Token(form='과', tag='JKB', start=406, len=1),\n",
       "   Token(form='대화', tag='NNG', start=408, len=2),\n",
       "   Token(form='하', tag='XSV', start=410, len=1),\n",
       "   Token(form='다', tag='EC', start=411, len=1),\n",
       "   Token(form='보', tag='VX', start=412, len=1),\n",
       "   Token(form='면', tag='EC', start=413, len=1),\n",
       "   Token(form='꼭', tag='MAG', start=415, len=1),\n",
       "   Token(form='일', tag='NNG', start=417, len=1),\n",
       "   Token(form='도', tag='JX', start=418, len=1),\n",
       "   Token(form='못', tag='MAG', start=420, len=1),\n",
       "   Token(form='하', tag='VV', start=421, len=1),\n",
       "   Token(form='면서', tag='EC', start=422, len=2),\n",
       "   Token(form='남', tag='NNG', start=425, len=1),\n",
       "   Token(form='에게', tag='JKB', start=426, len=2),\n",
       "   Token(form='책임', tag='NNG', start=429, len=2),\n",
       "   Token(form='만', tag='JX', start=431, len=1),\n",
       "   Token(form='저', tag='NP', start=433, len=1),\n",
       "   Token(form='이', tag='VCP', start=433, len=1),\n",
       "   Token(form='ᆫ가', tag='EC', start=433, len=2),\n",
       "   Token(form='하', tag='VV', start=435, len=1),\n",
       "   Token(form='는', tag='ETM', start=436, len=1),\n",
       "   Token(form='상사', tag='NNG', start=438, len=2),\n",
       "   Token(form='얘기', tag='NNG', start=441, len=2),\n",
       "   Token(form='가', tag='JKS', start=443, len=1),\n",
       "   Token(form='나오', tag='VV', start=445, len=2),\n",
       "   Token(form='ᆫ다', tag='EF', start=446, len=2),\n",
       "   Token(form='.', tag='SF', start=448, len=1),\n",
       "   Token(form='나', tag='NP', start=450, len=1),\n",
       "   Token(form='는', tag='JX', start=451, len=1),\n",
       "   Token(form='이런', tag='MM', start=453, len=2),\n",
       "   Token(form='어른', tag='NNG', start=456, len=2),\n",
       "   Token(form='이', tag='JKC', start=458, len=1),\n",
       "   Token(form='되', tag='VV', start=460, len=1),\n",
       "   Token(form='지', tag='EC', start=461, len=1),\n",
       "   Token(form='는', tag='JX', start=462, len=1),\n",
       "   Token(form='말', tag='VX', start=464, len=1),\n",
       "   Token(form='어야지', tag='EF', start=465, len=3),\n",
       "   Token(form='.', tag='SF', start=468, len=1),\n",
       "   Token(form='스스로', tag='NNG', start=470, len=3),\n",
       "   Token(form='를', tag='JKO', start=473, len=1),\n",
       "   Token(form='경계', tag='NNG', start=475, len=2),\n",
       "   Token(form='하', tag='XSV', start=477, len=1),\n",
       "   Token(form='기', tag='ETN', start=478, len=1),\n",
       "   Token(form='위하', tag='VV', start=480, len=2),\n",
       "   Token(form='어서', tag='EC', start=481, len=2),\n",
       "   Token(form='도', tag='JX', start=483, len=1),\n",
       "   Token(form='한', tag='MM', start=485, len=1),\n",
       "   Token(form='번', tag='NNB', start=487, len=1),\n",
       "   Token(form='쯤', tag='NNB', start=488, len=1),\n",
       "   Token(form='읽', tag='VV', start=490, len=1),\n",
       "   Token(form='어', tag='EC', start=491, len=1),\n",
       "   Token(form='보', tag='VX', start=492, len=1),\n",
       "   Token(form='면', tag='EC', start=493, len=1),\n",
       "   Token(form='좋', tag='VA', start=495, len=1),\n",
       "   Token(form='을', tag='ETM', start=496, len=1),\n",
       "   Token(form='책', tag='NNG', start=498, len=1),\n",
       "   Token(form='이', tag='VCP', start=499, len=1),\n",
       "   Token(form='다', tag='EF', start=500, len=1),\n",
       "   Token(form='.', tag='SF', start=501, len=1),\n",
       "   Token(form='중간중간', tag='NNG', start=503, len=4),\n",
       "   Token(form='뜨끔', tag='MAG', start=508, len=2),\n",
       "   Token(form='하', tag='VV', start=510, len=1),\n",
       "   Token(form='는', tag='ETM', start=511, len=1),\n",
       "   Token(form='문장', tag='NNG', start=513, len=2),\n",
       "   Token(form='이', tag='JKS', start=515, len=1),\n",
       "   Token(form='있', tag='VV', start=517, len=1),\n",
       "   Token(form='을', tag='ETM', start=518, len=1),\n",
       "   Token(form='수', tag='NNB', start=520, len=1),\n",
       "   Token(form='도', tag='JX', start=521, len=1),\n",
       "   Token(form='있', tag='VA', start=523, len=1),\n",
       "   Token(form='다', tag='EF', start=524, len=1),\n",
       "   Token(form='.', tag='SF', start=525, len=1),\n",
       "   Token(form='[', tag='SSO', start=526, len=1),\n",
       "   Token(form='당신', tag='NP', start=527, len=2),\n",
       "   Token(form='의', tag='JKG', start=529, len=1),\n",
       "   Token(form='모든', tag='MM', start=531, len=2),\n",
       "   Token(form='것', tag='NNB', start=534, len=1),\n",
       "   Token(form='이', tag='JKS', start=535, len=1),\n",
       "   Token(form='메시지', tag='NNG', start=537, len=3),\n",
       "   Token(form='이', tag='VCP', start=540, len=0),\n",
       "   Token(form='다', tag='EF', start=540, len=1),\n",
       "   Token(form=']', tag='SSC', start=541, len=1),\n",
       "   Token(form='책', tag='NNG', start=542, len=1),\n",
       "   Token(form='의', tag='JKG', start=543, len=1),\n",
       "   Token(form='부제', tag='NNG', start=545, len=2),\n",
       "   Token(form='이', tag='VCP', start=547, len=1),\n",
       "   Token(form='기', tag='ETN', start=548, len=1),\n",
       "   Token(form='도', tag='JX', start=549, len=1),\n",
       "   Token(form='하', tag='VV', start=551, len=1),\n",
       "   Token(form='ᆫ', tag='ETM', start=551, len=1),\n",
       "   Token(form='이', tag='MM', start=553, len=1),\n",
       "   Token(form='챕터', tag='NNG', start=555, len=2),\n",
       "   Token(form='가', tag='JKS', start=557, len=1),\n",
       "   Token(form='가장', tag='MAG', start=559, len=2),\n",
       "   Token(form='인상', tag='NNG', start=562, len=2),\n",
       "   Token(form='적', tag='XSN', start=564, len=1),\n",
       "   Token(form='이', tag='VCP', start=565, len=1),\n",
       "   Token(form='었', tag='EP', start=566, len=1),\n",
       "   Token(form='다', tag='EF', start=567, len=1),\n",
       "   Token(form='.', tag='SF', start=568, len=1),\n",
       "   Token(form='요즘', tag='NNG', start=570, len=2),\n",
       "   Token(form='들', tag='VV', start=573, len=1),\n",
       "   Token(form='어', tag='EC', start=574, len=1),\n",
       "   Token(form='기록', tag='NNG', start=576, len=2),\n",
       "   Token(form='의', tag='JKG', start=578, len=1),\n",
       "   Token(form='중요', tag='NNG', start=580, len=2),\n",
       "   Token(form='성', tag='XSN', start=582, len=1),\n",
       "   Token(form='을', tag='JKO', start=583, len=1),\n",
       "   Token(form='점점', tag='MAG', start=585, len=2),\n",
       "   Token(form='체감', tag='NNG', start=588, len=2),\n",
       "   Token(form='하', tag='XSV', start=590, len=1),\n",
       "   Token(form='고', tag='EC', start=591, len=1),\n",
       "   Token(form='있', tag='VX', start=593, len=1),\n",
       "   Token(form='다', tag='EF', start=594, len=1),\n",
       "   Token(form='.', tag='SF', start=595, len=1),\n",
       "   Token(form='개인', tag='NNG', start=597, len=2),\n",
       "   Token(form='적', tag='XSN', start=599, len=1),\n",
       "   Token(form='으로', tag='JKB', start=600, len=2),\n",
       "   Token(form='기억', tag='NNG', start=603, len=2),\n",
       "   Token(form='하', tag='XSV', start=605, len=1),\n",
       "   Token(form='기', tag='ETN', start=606, len=1),\n",
       "   Token(form='위하', tag='VV', start=608, len=2),\n",
       "   Token(form='ᆷ', tag='ETN', start=609, len=1),\n",
       "   Token(form='도', tag='JX', start=610, len=1),\n",
       "   Token(form='있', tag='VV', start=612, len=1),\n",
       "   Token(form='지만', tag='EC', start=613, len=2),\n",
       "   Token(form='어디', tag='NP', start=616, len=2),\n",
       "   Token(form='이', tag='VCP', start=617, len=1),\n",
       "   Token(form='ᆫ가', tag='EC', start=617, len=2),\n",
       "   Token(form='에', tag='JKB', start=619, len=1),\n",
       "   Token(form='나', tag='NP', start=621, len=1),\n",
       "   Token(form='가', tag='JKS', start=622, len=1),\n",
       "   Token(form='한', tag='MM', start=624, len=1),\n",
       "   Token(form='활동', tag='NNG', start=626, len=2),\n",
       "   Token(form='들', tag='XSN', start=628, len=1),\n",
       "   Token(form='과', tag='JC', start=629, len=1),\n",
       "   Token(form='나', tag='NP', start=631, len=1),\n",
       "   Token(form='의', tag='JKG', start=631, len=1),\n",
       "   Token(form='생각', tag='NNG', start=633, len=2),\n",
       "   Token(form='을', tag='JKO', start=635, len=1),\n",
       "   Token(form='보이', tag='VV', start=637, len=2),\n",
       "   Token(form='어', tag='EC', start=638, len=1),\n",
       "   Token(form='주', tag='VX', start=639, len=1),\n",
       "   Token(form='어야', tag='EC', start=639, len=2),\n",
       "   Token(form='하', tag='VX', start=641, len=1),\n",
       "   Token(form='ᆯ', tag='ETM', start=641, len=1),\n",
       "   Token(form='때', tag='NNG', start=643, len=1),\n",
       "   Token(form='특히', tag='MAG', start=645, len=2),\n",
       "   Token(form='더', tag='MAG', start=648, len=1),\n",
       "   Token(form='절실히', tag='MAG', start=650, len=3),\n",
       "   Token(form='느끼', tag='VV', start=654, len=2),\n",
       "   Token(form='ᆫ', tag='ETM', start=655, len=1),\n",
       "   Token(form='다.', tag='SB', start=656, len=2),\n",
       "   Token(form='지금', tag='NNG', start=659, len=2),\n",
       "   Token(form='까지', tag='JX', start=661, len=2),\n",
       "   Token(form='는', tag='JX', start=663, len=1),\n",
       "   Token(form='그거', tag='NP', start=665, len=2),\n",
       "   Token(form='ᆯ', tag='JKO', start=666, len=1),\n",
       "   Token(form='기록', tag='NNG', start=668, len=2),\n",
       "   Token(form='하', tag='XSV', start=670, len=1),\n",
       "   Token(form='어야', tag='EC', start=670, len=2),\n",
       "   Token(form='하', tag='VX', start=673, len=1),\n",
       "   Token(form='ᆫ다는', tag='ETM', start=673, len=3),\n",
       "   Token(form='생각', tag='NNG', start=677, len=2),\n",
       "   Token(form='을', tag='JKO', start=679, len=1),\n",
       "   Token(form='못', tag='MAG', start=681, len=1),\n",
       "   Token(form='하', tag='VV', start=682, len=1),\n",
       "   Token(form='었', tag='EP', start=682, len=1),\n",
       "   Token(form='는데', tag='EC', start=683, len=2),\n",
       "   Token(form='이제', tag='MAG', start=686, len=2),\n",
       "   Token(form='오', tag='VV', start=689, len=1),\n",
       "   Token(form='어서', tag='EC', start=689, len=2),\n",
       "   Token(form='정리', tag='NNG', start=692, len=2),\n",
       "   Token(form='하', tag='XSV', start=694, len=1),\n",
       "   Token(form='려고', tag='EC', start=695, len=2),\n",
       "   Token(form='보', tag='VX', start=698, len=1),\n",
       "   Token(form='니', tag='EC', start=699, len=1),\n",
       "   Token(form='나', tag='NP', start=701, len=1),\n",
       "   Token(form='가', tag='JKS', start=702, len=1),\n",
       "   Token(form='하', tag='VV', start=704, len=1),\n",
       "   Token(form='었', tag='EP', start=704, len=1),\n",
       "   Token(form='던', tag='ETM', start=705, len=1),\n",
       "   Token(form='것', tag='NNB', start=707, len=1),\n",
       "   Token(form='이', tag='JKS', start=707, len=1),\n",
       "   Token(form='뭐', tag='NP', start=709, len=1),\n",
       "   Token(form='이', tag='VCP', start=709, len=1),\n",
       "   Token(form='ᆫ지', tag='EC', start=709, len=2),\n",
       "   Token(form=',', tag='SP', start=711, len=1),\n",
       "   Token(form='그거', tag='NP', start=713, len=2),\n",
       "   Token(form='ᆯ', tag='JKO', start=714, len=1),\n",
       "   Token(form='통하', tag='VV', start=716, len=2),\n",
       "   Token(form='어', tag='EC', start=717, len=1),\n",
       "   Token(form='뭐', tag='NP', start=719, len=1),\n",
       "   Token(form='ᆯ', tag='JKO', start=719, len=1),\n",
       "   Token(form='배우', tag='VV', start=721, len=2),\n",
       "   Token(form='고', tag='EC', start=723, len=1),\n",
       "   Token(form='느끼', tag='VV', start=725, len=2),\n",
       "   Token(form='었', tag='EP', start=726, len=1),\n",
       "   Token(form='는지', tag='EC', start=727, len=2),\n",
       "   Token(form='잘', tag='MAG', start=730, len=1),\n",
       "   Token(form='기억', tag='NNG', start=732, len=2),\n",
       "   Token(form='이', tag='JKS', start=734, len=1),\n",
       "   Token(form='안', tag='MAG', start=736, len=1),\n",
       "   Token(form='나', tag='VV', start=738, len=1),\n",
       "   Token(form='어서', tag='EC', start=738, len=2),\n",
       "   Token(form='막막', tag='XR', start=741, len=2),\n",
       "   Token(form='하', tag='XSA', start=743, len=1),\n",
       "   Token(form='었', tag='EP', start=743, len=1),\n",
       "   Token(form='다', tag='EF', start=744, len=1),\n",
       "   Token(form='.', tag='SF', start=745, len=1),\n",
       "   Token(form='그제서야', tag='MAG', start=747, len=4),\n",
       "   Token(form='어떤', tag='MM', start=752, len=2),\n",
       "   Token(form='형태', tag='NNG', start=755, len=2),\n",
       "   Token(form='로', tag='JKB', start=757, len=1),\n",
       "   Token(form='이', tag='VCP', start=758, len=0),\n",
       "   Token(form='든', tag='EC', start=758, len=1),\n",
       "   Token(form='기록', tag='NNG', start=760, len=2),\n",
       "   Token(form='을', tag='JKO', start=762, len=1),\n",
       "   Token(form='하', tag='VV', start=764, len=1),\n",
       "   Token(form='어야', tag='EC', start=764, len=2),\n",
       "   Token(form='겠', tag='EP', start=766, len=1),\n",
       "   Token(form='구나', tag='EC', start=767, len=2),\n",
       "   Token(form='싶', tag='VX', start=770, len=1),\n",
       "   Token(form='었', tag='EP', start=771, len=1),\n",
       "   Token(form='다', tag='EF', start=772, len=1),\n",
       "   Token(form='.', tag='SF', start=773, len=1),\n",
       "   Token(form='이제', tag='MAG', start=775, len=2),\n",
       "   Token(form='더', tag='MAG', start=778, len=1),\n",
       "   Token(form='는', tag='JX', start=779, len=1),\n",
       "   Token(form='학벌', tag='NNG', start=781, len=2),\n",
       "   Token(form='이나', tag='JC', start=783, len=2),\n",
       "   Token(form='성적', tag='NNG', start=786, len=2),\n",
       "   Token(form='이', tag='JKS', start=788, len=1),\n",
       "   Token(form='나', tag='NP', start=790, len=1),\n",
       "   Token(form='를', tag='JKO', start=791, len=1),\n",
       "   Token(form='증명', tag='NNG', start=793, len=2),\n",
       "   Token(form='하', tag='XSV', start=795, len=1),\n",
       "   Token(form='어', tag='EC', start=795, len=1),\n",
       "   Token(form='주', tag='VX', start=796, len=1),\n",
       "   Token(form='지', tag='EC', start=797, len=1),\n",
       "   Token(form='않', tag='VX', start=799, len=1),\n",
       "   Token(form='는다', tag='EF', start=800, len=2),\n",
       "   Token(form='.', tag='SF', start=802, len=1),\n",
       "   Token(form='요즘', tag='NNG', start=804, len=2),\n",
       "   Token(form='들', tag='VV', start=807, len=1),\n",
       "   Token(form='어', tag='EC', start=808, len=1),\n",
       "   Token(form='그거', tag='NP', start=810, len=2),\n",
       "   Token(form='ᆯ', tag='JKO', start=811, len=1),\n",
       "   Token(form='정말', tag='MAG', start=813, len=2),\n",
       "   Token(form='많이', tag='MAG', start=816, len=2),\n",
       "   Token(form='느끼', tag='VV', start=819, len=2),\n",
       "   Token(form='고', tag='EC', start=821, len=1),\n",
       "   Token(form='있', tag='VX', start=823, len=1),\n",
       "   Token(form='고', tag='EC', start=824, len=1),\n",
       "   Token(form=',', tag='SP', start=825, len=1),\n",
       "   Token(form='앞', tag='NNG', start=827, len=1),\n",
       "   Token(form='으로', tag='JKB', start=828, len=2),\n",
       "   Token(form='는', tag='JX', start=830, len=1),\n",
       "   Token(form='세상', tag='NNG', start=832, len=2),\n",
       "   Token(form='이', tag='JKS', start=834, len=1),\n",
       "   Token(form='더욱', tag='MAG', start=836, len=2),\n",
       "   Token(form='그런', tag='MM', start=839, len=2),\n",
       "   Token(form='방향', tag='NNG', start=842, len=2),\n",
       "   Token(form='으로', tag='JKB', start=844, len=2),\n",
       "   Token(form='변하', tag='VV', start=847, len=2),\n",
       "   Token(form='어', tag='EC', start=848, len=1),\n",
       "   Token(form='가', tag='VX', start=849, len=1),\n",
       "   Token(form='ᆯ', tag='ETM', start=849, len=1),\n",
       "   Token(form='것', tag='NNB', start=851, len=1),\n",
       "   Token(form='이', tag='VCP', start=852, len=1),\n",
       "   Token(form='라', tag='EC', start=853, len=1),\n",
       "   Token(form='생각', tag='NNG', start=855, len=2),\n",
       "   Token(form='하', tag='XSV', start=857, len=1),\n",
       "   Token(form='ᆫ다', tag='EF', start=857, len=2),\n",
       "   Token(form='.', tag='SF', start=859, len=1),\n",
       "   Token(form='저자', tag='NNG', start=860, len=2),\n",
       "   Token(form='는', tag='JX', start=862, len=1),\n",
       "   Token(form='일상', tag='NNG', start=864, len=2),\n",
       "   Token(form='의', tag='JKG', start=866, len=1),\n",
       "   Token(form='모든', tag='MM', start=868, len=2),\n",
       "   Token(form='기록', tag='NNG', start=871, len=2),\n",
       "   Token(form='들', tag='XSN', start=873, len=1),\n",
       "   Token(form='이', tag='JKS', start=874, len=1),\n",
       "   Token(form='자신', tag='NNG', start=876, len=2),\n",
       "   Token(form='을', tag='JKO', start=878, len=1),\n",
       "   Token(form='표현', tag='NNG', start=880, len=2),\n",
       "   Token(form='하', tag='XSV', start=882, len=1),\n",
       "   Token(form='는', tag='ETM', start=883, len=1),\n",
       "   Token(form='메시지', tag='NNG', start=885, len=3),\n",
       "   Token(form='가', tag='JKS', start=888, len=1),\n",
       "   Token(form='되', tag='VV', start=890, len=1),\n",
       "   Token(form='ᆫ다고', tag='EC', start=890, len=3),\n",
       "   Token(form='말', tag='NNG', start=894, len=1),\n",
       "   Token(form='하', tag='XSV', start=895, len=1),\n",
       "   Token(form='ᆫ다', tag='EF', start=895, len=2),\n",
       "   Token(form='.', tag='SF', start=897, len=1),\n",
       "   Token(form='이력서', tag='NNG', start=899, len=3),\n",
       "   Token(form='나', tag='JC', start=902, len=1),\n",
       "   Token(form='포트폴리오', tag='NNG', start=904, len=5),\n",
       "   Token(form='로', tag='JKB', start=909, len=1),\n",
       "   Token(form='자신', tag='NNG', start=911, len=2),\n",
       "   Token(form='을', tag='JKO', start=913, len=1),\n",
       "   Token(form='소개', tag='NNG', start=915, len=2),\n",
       "   Token(form='하', tag='XSV', start=917, len=1),\n",
       "   Token(form='는', tag='ETM', start=918, len=1),\n",
       "   Token(form='것', tag='NNB', start=920, len=1),\n",
       "   Token(form='을', tag='JKO', start=921, len=1),\n",
       "   Token(form='넘', tag='VV', start=923, len=1),\n",
       "   Token(form='어', tag='EC', start=924, len=1),\n",
       "   Token(form='이제', tag='NNG', start=926, len=2),\n",
       "   Token(form='는', tag='JX', start=928, len=1),\n",
       "   Token(form='소비', tag='NNG', start=930, len=2),\n",
       "   Token(form='로', tag='JKB', start=932, len=1),\n",
       "   Token(form='도', tag='JX', start=933, len=1),\n",
       "   Token(form=',', tag='SP', start=934, len=1),\n",
       "   Token(form='sns', tag='SL', start=936, len=3),\n",
       "   Token(form='관계', tag='NNG', start=939, len=2),\n",
       "   Token(form='망', tag='NNG', start=941, len=1),\n",
       "   Token(form='으', tag='NNG', start=942, len=1),\n",
       "   Token(form='도', tag='JX', start=943, len=1),\n",
       "   Token(form='나', tag='NP', start=945, len=1),\n",
       "   Token(form='를', tag='JKO', start=946, len=1),\n",
       "   Token(form='표현', tag='NNG', start=948, len=2),\n",
       "   Token(form='하', tag='XSV', start=950, len=1),\n",
       "   Token(form='는', tag='ETM', start=951, len=1),\n",
       "   Token(form='시대', tag='NNG', start=953, len=2),\n",
       "   Token(form='가', tag='JKC', start=955, len=1),\n",
       "   Token(form='오', tag='VV', start=957, len=1),\n",
       "   Token(form='ᆫ', tag='ETM', start=957, len=1),\n",
       "   Token(form='것', tag='NNB', start=959, len=1),\n",
       "   Token(form='이', tag='VCP', start=960, len=1),\n",
       "   Token(form='다', tag='EF', start=961, len=1),\n",
       "   Token(form='.', tag='SF', start=962, len=1),\n",
       "   Token(form='사람', tag='NNG', start=964, len=2),\n",
       "   Token(form='들', tag='XSN', start=966, len=1),\n",
       "   Token(form='은', tag='JX', start=967, len=1),\n",
       "   Token(form='이제', tag='MAG', start=969, len=2),\n",
       "   Token(form='필요', tag='NNG', start=972, len=2),\n",
       "   Token(form='에', tag='JKB', start=974, len=1),\n",
       "   Token(form='의하', tag='VV', start=976, len=2),\n",
       "   Token(form='어', tag='EC', start=977, len=1),\n",
       "   Token(form='구매', tag='NNG', start=979, len=2),\n",
       "   Token(form='하', tag='XSV', start=981, len=1),\n",
       "   Token(form='는', tag='ETM', start=982, len=1),\n",
       "   Token(form='물질', tag='NNG', start=984, len=2),\n",
       "   Token(form='소비', tag='NNG', start=986, len=2),\n",
       "   Token(form='가', tag='JKS', start=988, len=1),\n",
       "   Token(form='아니', tag='VCN', start=990, len=2),\n",
       "   Token(form='ᆫ', tag='ETM', start=991, len=1),\n",
       "   Token(form=',', tag='SP', start=992, len=1),\n",
       "   Token(form='자신', tag='NNG', start=994, len=2),\n",
       "   Token(form='의', tag='JKG', start=996, len=1),\n",
       "   Token(form='개성', tag='NNG', start=998, len=2),\n",
       "   Token(form='을', tag='JKO', start=1000, len=1),\n",
       "   Token(form='드러내', tag='VV', start=1002, len=3),\n",
       "   Token(form='고', tag='EC', start=1005, len=1),\n",
       "   Token(form='의미', tag='NNG', start=1007, len=2),\n",
       "   Token(form='를', tag='JKO', start=1009, len=1),\n",
       "   Token(form='표현', tag='NNG', start=1011, len=2),\n",
       "   Token(form='하', tag='XSV', start=1013, len=1),\n",
       "   Token(form='기', tag='ETN', start=1014, len=1),\n",
       "   Token(form='위하', tag='VV', start=1016, len=2),\n",
       "   Token(form='ᆫ', tag='ETM', start=1017, len=1),\n",
       "   Token(form='의미', tag='NNG', start=1019, len=2),\n",
       "   Token(form='소비', tag='NNG', start=1021, len=2),\n",
       "   Token(form='를', tag='JKO', start=1023, len=1),\n",
       "   Token(form='하', tag='VV', start=1025, len=1),\n",
       "   Token(form='ᆫ다', tag='EF', start=1025, len=2),\n",
       "   Token(form='.', tag='SF', start=1027, len=1),\n",
       "   Token(form='나', tag='NP', start=1029, len=1),\n",
       "   Token(form='또한', tag='MAJ', start=1031, len=2),\n",
       "   Token(form='요즘', tag='NNG', start=1034, len=2),\n",
       "   Token(form='들', tag='VV', start=1037, len=1),\n",
       "   Token(form='어', tag='EC', start=1038, len=1),\n",
       "   Token(form='소비', tag='NNG', start=1040, len=2),\n",
       "   Token(form='패턴', tag='NNG', start=1043, len=2),\n",
       "   Token(form='이', tag='JKS', start=1045, len=1),\n",
       "   Token(form='이런', tag='MM', start=1047, len=2),\n",
       "   Token(form='식', tag='NNB', start=1050, len=1),\n",
       "   Token(form='으로', tag='JKB', start=1051, len=2),\n",
       "   Token(form='변하', tag='VV', start=1054, len=2),\n",
       "   Token(form='고', tag='EC', start=1056, len=1),\n",
       "   Token(form='있', tag='VX', start=1058, len=1),\n",
       "   Token(form='다', tag='EF', start=1059, len=1),\n",
       "   Token(form='.', tag='SF', start=1060, len=1),\n",
       "   Token(form='같', tag='VA', start=1062, len=1),\n",
       "   Token(form='은', tag='ETM', start=1063, len=1),\n",
       "   Token(form='물건', tag='NNG', start=1065, len=2),\n",
       "   Token(form='이', tag='VCP', start=1067, len=1),\n",
       "   Token(form='더라도', tag='EC', start=1068, len=3),\n",
       "   Token(form='브랜드', tag='NNG', start=1072, len=3),\n",
       "   Token(form='가', tag='JKS', start=1075, len=1),\n",
       "   Token(form='추구', tag='NNG', start=1077, len=2),\n",
       "   Token(form='하', tag='XSV', start=1079, len=1),\n",
       "   Token(form='는', tag='ETM', start=1080, len=1),\n",
       "   Token(form='가치', tag='NNG', start=1082, len=2),\n",
       "   Token(form='가', tag='JKS', start=1084, len=1),\n",
       "   Token(form='나', tag='NP', start=1086, len=1),\n",
       "   Token(form='가', tag='JKS', start=1087, len=1),\n",
       "   Token(form='지향', tag='NNG', start=1089, len=2),\n",
       "   Token(form='하', tag='XSV', start=1091, len=1),\n",
       "   Token(form='는', tag='ETM', start=1092, len=1),\n",
       "   Token(form='바', tag='NNB', start=1094, len=1),\n",
       "   Token(form='와', tag='JKB', start=1095, len=1),\n",
       "   Token(form='맞', tag='VV', start=1097, len=1),\n",
       "   Token(form='는', tag='ETM', start=1098, len=1),\n",
       "   Token(form='경우', tag='NNG', start=1100, len=2),\n",
       "   Token(form='에', tag='JKB', start=1102, len=1),\n",
       "   Token(form='는', tag='JX', start=1103, len=1),\n",
       "   Token(form='더', tag='MAG', start=1105, len=1),\n",
       "   Token(form='즐겁', tag='VA-I', start=1107, len=3),\n",
       "   Token(form='은', tag='ETM', start=1109, len=1),\n",
       "   Token(form='마음', tag='NNG', start=1111, len=2),\n",
       "   Token(form='으로', tag='JKB', start=1113, len=2),\n",
       "   Token(form='소비', tag='NNG', start=1116, len=2),\n",
       "   Token(form='를', tag='JKO', start=1118, len=1),\n",
       "   Token(form='하', tag='VV', start=1120, len=1),\n",
       "   Token(form='게', tag='EC', start=1121, len=1),\n",
       "   Token(form='되', tag='VV', start=1123, len=1),\n",
       "   Token(form='고', tag='EC', start=1124, len=1),\n",
       "   Token(form='해당', tag='NNG', start=1126, len=2),\n",
       "   Token(form='브랜드', tag='NNG', start=1129, len=3),\n",
       "   Token(form='를', tag='JKO', start=1132, len=1),\n",
       "   Token(form='기억', tag='NNG', start=1134, len=2),\n",
       "   Token(form='하', tag='XSV', start=1136, len=1),\n",
       "   Token(form='게', tag='EC', start=1137, len=1),\n",
       "   Token(form='되', tag='VV', start=1139, len=1),\n",
       "   Token(form='는', tag='ETM', start=1140, len=1),\n",
       "   Token(form='것', tag='NNB', start=1142, len=1),\n",
       "   Token(form='같', tag='VA', start=1144, len=1),\n",
       "   Token(form='다', tag='EF', start=1145, len=1),\n",
       "   Token(form='.', tag='SF', start=1146, len=1),\n",
       "   Token(form='콘텐츠', tag='NNG', start=1147, len=3),\n",
       "   Token(form='소비', tag='NNG', start=1151, len=2),\n",
       "   Token(form='또한', tag='MAG', start=1154, len=2),\n",
       "   Token(form='마찬가지', tag='NNG', start=1157, len=4),\n",
       "   Token(form='로', tag='JKB', start=1161, len=1),\n",
       "   Token(form='나', tag='NP', start=1163, len=1),\n",
       "   Token(form='가', tag='JKS', start=1164, len=1),\n",
       "   Token(form='어떤', tag='MM', start=1166, len=2),\n",
       "   Token(form='콘텐츠', tag='NNG', start=1169, len=3),\n",
       "   Token(form='를', tag='JKO', start=1172, len=1),\n",
       "   Token(form='선택', tag='NNG', start=1174, len=2),\n",
       "   Token(form='하', tag='XSV', start=1176, len=1),\n",
       "   Token(form='느냐', tag='EC', start=1177, len=2),\n",
       "   Token(form='는', tag='JX', start=1179, len=1),\n",
       "   Token(form='곧', tag='MAG', start=1181, len=1),\n",
       "   Token(form='나', tag='NP', start=1183, len=1),\n",
       "   Token(form='의', tag='JKG', start=1184, len=1),\n",
       "   Token(form='취향', tag='NNG', start=1186, len=2),\n",
       "   Token(form='을', tag='JKO', start=1188, len=1),\n",
       "   Token(form='은연중', tag='NNG', start=1190, len=3),\n",
       "   Token(form='에', tag='JKB', start=1193, len=1),\n",
       "   Token(form='알리', tag='VV', start=1195, len=2),\n",
       "   Token(form='는', tag='ETM', start=1197, len=1),\n",
       "   Token(form='것', tag='NNB', start=1199, len=1),\n",
       "   Token(form='과', tag='JKB', start=1200, len=1),\n",
       "   Token(form='도', tag='JX', start=1201, len=1),\n",
       "   Token(form='같', tag='VA', start=1203, len=1),\n",
       "   Token(form='다', tag='EF', start=1204, len=1),\n",
       "   Token(form='.', tag='SF', start=1205, len=1),\n",
       "   Token(form='나', tag='NP', start=1207, len=1),\n",
       "   Token(form='의', tag='JKG', start=1208, len=1),\n",
       "   Token(form='sns', tag='SL', start=1210, len=3),\n",
       "   Token(form='에서', tag='JKB', start=1213, len=2),\n",
       "   Token(form='누구', tag='NP', start=1216, len=2),\n",
       "   Token(form='를', tag='JKO', start=1218, len=1),\n",
       "   Token(form='팔로우', tag='NNG', start=1220, len=3),\n",
       "   Token(form='하', tag='XSV', start=1223, len=1),\n",
       "   Token(form='느냐', tag='EC', start=1224, len=2),\n",
       "   Token(form='도', tag='JX', start=1226, len=1),\n",
       "   Token(form='나', tag='NP', start=1228, len=1),\n",
       "   Token(form='의', tag='JKG', start=1229, len=1),\n",
       "   Token(form='관심사', tag='NNG', start=1231, len=3),\n",
       "   Token(form='와', tag='JC', start=1234, len=1),\n",
       "   Token(form='네트워크', tag='NNG', start=1236, len=4),\n",
       "   Token(form='를', tag='JKO', start=1240, len=1),\n",
       "   Token(form='공개', tag='NNG', start=1242, len=2),\n",
       "   Token(form='하', tag='XSV', start=1244, len=1),\n",
       "   Token(form='는', tag='ETM', start=1245, len=1),\n",
       "   Token(form='행위', tag='NNG', start=1247, len=2),\n",
       "   Token(form='의', tag='JKG', start=1249, len=1),\n",
       "   Token(form='일종', tag='NNG', start=1251, len=2),\n",
       "   Token(form='이', tag='VCP', start=1253, len=1),\n",
       "   Token(form='ᆫ', tag='ETM', start=1253, len=1),\n",
       "   Token(form='셈', tag='NNB', start=1255, len=1),\n",
       "   Token(form='이', tag='VCP', start=1256, len=1),\n",
       "   Token(form='다', tag='EF', start=1257, len=1),\n",
       "   Token(form='.', tag='SF', start=1258, len=1),\n",
       "   Token(form='저자', tag='NNG', start=1260, len=2),\n",
       "   Token(form='는', tag='JX', start=1262, len=1),\n",
       "   Token(form='결국', tag='NNG', start=1264, len=2),\n",
       "   Token(form='이', tag='MM', start=1267, len=1),\n",
       "   Token(form='모든', tag='MM', start=1269, len=2),\n",
       "   Token(form='것', tag='NNB', start=1272, len=1),\n",
       "   Token(form='이', tag='JKS', start=1273, len=1),\n",
       "   Token(form='수렴', tag='NNG', start=1275, len=2),\n",
       "   Token(form='되', tag='XSV', start=1277, len=1),\n",
       "   Token(form='ᆫ', tag='ETM', start=1277, len=1),\n",
       "   Token(form='나', tag='NP', start=1279, len=1),\n",
       "   Token(form='의', tag='JKG', start=1280, len=1),\n",
       "   Token(form='라이프', tag='NNG', start=1282, len=3),\n",
       "   Token(form='스타일', tag='NNG', start=1285, len=3),\n",
       "   Token(form=',', tag='SP', start=1288, len=1),\n",
       "   Token(form='나', tag='NP', start=1290, len=1),\n",
       "   Token(form='의', tag='JKG', start=1291, len=1),\n",
       "   Token(form='모든', tag='MM', start=1293, len=2),\n",
       "   Token(form='것', tag='NNB', start=1296, len=1),\n",
       "   Token(form='이', tag='JKS', start=1297, len=1),\n",
       "   Token(form='메시지', tag='NNG', start=1299, len=3),\n",
       "   Token(form='가', tag='JKS', start=1302, len=1),\n",
       "   Token(form='되', tag='VV', start=1304, len=1),\n",
       "   Token(form='ᆫ다고', tag='EC', start=1304, len=3),\n",
       "   Token(form='말', tag='NNG', start=1308, len=1),\n",
       "   Token(form='하', tag='XSV', start=1309, len=1),\n",
       "   Token(form='고', tag='EC', start=1310, len=1),\n",
       "   Token(form='있', tag='VX', start=1312, len=1),\n",
       "   Token(form='다', tag='EF', start=1313, len=1),\n",
       "   Token(form='.', tag='SF', start=1314, len=1),\n",
       "   Token(form='한편', tag='NNG', start=1315, len=2),\n",
       "   Token(form='으로', tag='JKB', start=1317, len=2),\n",
       "   Token(form='는', tag='JX', start=1319, len=1),\n",
       "   Token(form='나', tag='NP', start=1321, len=1),\n",
       "   Token(form='의', tag='JKG', start=1322, len=1),\n",
       "   Token(form='모든', tag='MM', start=1324, len=2),\n",
       "   Token(form='기록', tag='NNG', start=1327, len=2),\n",
       "   Token(form='이', tag='JKS', start=1329, len=1),\n",
       "   Token(form='나', tag='NP', start=1331, len=1),\n",
       "   Token(form='를', tag='JKO', start=1332, len=1),\n",
       "   Token(form='보이', tag='VV', start=1334, len=2),\n",
       "   Token(form='어', tag='EC', start=1335, len=1),\n",
       "   Token(form='주', tag='VX', start=1336, len=1),\n",
       "   Token(form='는', tag='ETM', start=1337, len=1),\n",
       "   Token(form='수단', tag='NNG', start=1339, len=2),\n",
       "   Token(form='이', tag='JKC', start=1341, len=1),\n",
       "   Token(form='되', tag='VV', start=1343, len=1),\n",
       "   Token(form='ᆫ다는', tag='ETM', start=1343, len=3),\n",
       "   Token(form='것', tag='NNB', start=1347, len=1),\n",
       "   Token(form='이', tag='JKS', start=1347, len=1),\n",
       "   Token(form='조금', tag='MAG', start=1349, len=2),\n",
       "   Token(form='숨', tag='NNG', start=1352, len=1),\n",
       "   Token(form='막히', tag='VV', start=1353, len=2),\n",
       "   Token(form='기', tag='ETN', start=1355, len=1),\n",
       "   Token(form='도', tag='JX', start=1356, len=1),\n",
       "   Token(form='하', tag='VX', start=1358, len=1),\n",
       "   Token(form='고', tag='EC', start=1359, len=1),\n",
       "   Token(form='무섭', tag='VA-I', start=1361, len=2),\n",
       "   Token(form='게', tag='EC', start=1363, len=1),\n",
       "   Token(form='느끼', tag='VV', start=1365, len=2),\n",
       "   Token(form='어', tag='EC', start=1366, len=1),\n",
       "   Token(form='지', tag='VX', start=1367, len=1),\n",
       "   Token(form='기', tag='ETN', start=1368, len=1),\n",
       "   Token(form='도', tag='JX', start=1369, len=1),\n",
       "   Token(form='하', tag='VX', start=1371, len=1),\n",
       "   Token(form='ᆫ다', tag='EF', start=1371, len=2),\n",
       "   Token(form='.', tag='SF', start=1373, len=1),\n",
       "   Token(form='비밀', tag='NNG', start=1375, len=2),\n",
       "   Token(form='이', tag='JKS', start=1377, len=1),\n",
       "   Token(form='없', tag='VA', start=1379, len=1),\n",
       "   Token(form='는', tag='ETM', start=1380, len=1),\n",
       "   Token(form='사회', tag='NNG', start=1382, len=2),\n",
       "   Token(form='가', tag='JKC', start=1384, len=1),\n",
       "   Token(form='되', tag='VV', start=1386, len=1),\n",
       "   Token(form='어', tag='EC', start=1387, len=1),\n",
       "   Token(form='가', tag='VX', start=1388, len=1),\n",
       "   Token(form='고', tag='EC', start=1389, len=1),\n",
       "   Token(form='있', tag='VX', start=1391, len=1),\n",
       "   Token(form='는', tag='ETM', start=1392, len=1),\n",
       "   Token(form='것', tag='NNB', start=1394, len=1),\n",
       "   Token(form='같', tag='VA', start=1396, len=1),\n",
       "   Token(form='다', tag='EF', start=1397, len=1),\n",
       "   Token(form='.', tag='SF', start=1398, len=1),\n",
       "   Token(form='나', tag='NP', start=1400, len=1),\n",
       "   Token(form='는', tag='JX', start=1401, len=1),\n",
       "   Token(form='지금', tag='NNG', start=1403, len=2),\n",
       "   Token(form='까지', tag='JX', start=1405, len=2),\n",
       "   Token(form='현실', tag='NNG', start=1408, len=2),\n",
       "   Token(form='에', tag='JKB', start=1410, len=1),\n",
       "   Token(form='안주', tag='NNG', start=1412, len=2),\n",
       "   Token(form='하', tag='XSV', start=1414, len=1),\n",
       "   Token(form='며', tag='EC', start=1415, len=1),\n",
       "   Token(form='변화', tag='NNG', start=1417, len=2),\n",
       "   Token(form='를', tag='JKO', start=1419, len=1),\n",
       "   Token(form='두려워하', tag='VV', start=1421, len=4),\n",
       "   Token(form='고', tag='EC', start=1425, len=1),\n",
       "   Token(form='거부', tag='NNG', start=1427, len=2),\n",
       "   Token(form='하', tag='XSV', start=1429, len=1),\n",
       "   Token(form='어', tag='EC', start=1429, len=1),\n",
       "   Token(form='오', tag='VX', start=1430, len=1),\n",
       "   Token(form='었', tag='EP', start=1430, len=1),\n",
       "   Token(form='다', tag='EF', start=1431, len=1),\n",
       "   Token(form='.', tag='SF', start=1432, len=1),\n",
       "   Token(form='어쩌면', tag='MAG', start=1434, len=3),\n",
       "   Token(form='나', tag='NP', start=1438, len=1),\n",
       "   Token(form='는', tag='JX', start=1439, len=1),\n",
       "   Token(form='그', tag='MM', start=1441, len=1),\n",
       "   Token(form='변화', tag='NNG', start=1443, len=2),\n",
       "   Token(form='에', tag='JKB', start=1445, len=1),\n",
       "   Token(form='해당', tag='NNG', start=1447, len=2),\n",
       "   Token(form='되', tag='XSV', start=1449, len=1),\n",
       "   Token(form='지', tag='EC', start=1450, len=1),\n",
       "   Token(form='않', tag='VX', start=1452, len=1),\n",
       "   Token(form='을', tag='ETM', start=1453, len=1),\n",
       "   Token(form='거', tag='NNB', start=1455, len=1),\n",
       "   Token(form='이', tag='VCP', start=1456, len=0),\n",
       "   Token(form='라고', tag='EC', start=1456, len=2),\n",
       "   Token(form='생각', tag='NNG', start=1459, len=2),\n",
       "   Token(form='하', tag='XSV', start=1461, len=1),\n",
       "   Token(form='었', tag='EP', start=1461, len=1),\n",
       "   Token(form='던', tag='ETM', start=1462, len=1),\n",
       "   Token(form='것', tag='NNB', start=1464, len=1),\n",
       "   Token(form='같', tag='VA', start=1466, len=1),\n",
       "   Token(form='기', tag='ETN', start=1467, len=1),\n",
       "   Token(form='도', tag='JX', start=1468, len=1),\n",
       "   Token(form='하', tag='VX', start=1470, len=1),\n",
       "   Token(form='다', tag='EF', start=1471, len=1),\n",
       "   Token(form='.', tag='SF', start=1472, len=1),\n",
       "   Token(form='그러나', tag='MAJ', start=1474, len=3),\n",
       "   Token(form='저자', tag='NNG', start=1478, len=2),\n",
       "   Token(form='가', tag='JKS', start=1480, len=1),\n",
       "   Token(form='서문', tag='NNG', start=1482, len=2),\n",
       "   Token(form='에서', tag='JKB', start=1484, len=2),\n",
       "   Token(form='이야기', tag='NNG', start=1487, len=3),\n",
       "   Token(form='하', tag='XSV', start=1490, len=1),\n",
       "   Token(form='었', tag='EP', start=1490, len=1),\n",
       "   Token(form='듯이', tag='EC', start=1491, len=2),\n",
       "   Token(form='아직', tag='MAG', start=1494, len=2),\n",
       "   Token(form='나', tag='NP', start=1497, len=1),\n",
       "   Token(form='에게', tag='JKB', start=1498, len=2),\n",
       "   Token(form='일어나', tag='VV', start=1501, len=3),\n",
       "   Token(form='지', tag='EC', start=1504, len=1),\n",
       "   Token(form='않', tag='VX', start=1506, len=1),\n",
       "   Token(form='었', tag='EP', start=1507, len=1),\n",
       "   Token(form='다', tag='EC', start=1508, len=1),\n",
       "   Token(form='하', tag='VV', start=1510, len=1),\n",
       "   Token(form='어도', tag='EC', start=1510, len=2),\n",
       "   Token(form='변화', tag='NNG', start=1513, len=2),\n",
       "   Token(form='는', tag='JX', start=1515, len=1),\n",
       "   Token(form='분명히', tag='MAG', start=1517, len=3),\n",
       "   Token(form='일어나', tag='VV', start=1521, len=3),\n",
       "   Token(form='고', tag='EC', start=1524, len=1),\n",
       "   Token(form='있', tag='VX', start=1526, len=1),\n",
       "   Token(form='다', tag='EF', start=1527, len=1),\n",
       "   Token(form='.', tag='SF', start=1528, len=1),\n",
       "   Token(form='언젠가', tag='MAG', start=1530, len=3),\n",
       "   Token(form='는', tag='JX', start=1533, len=1),\n",
       "   Token(form='나', tag='NP', start=1535, len=1),\n",
       "   Token(form='에게', tag='JKB', start=1536, len=2),\n",
       "   Token(form='도', tag='JX', start=1538, len=1),\n",
       "   Token(form='올', tag='MM', start=1540, len=1),\n",
       "   Token(form='그', tag='MM', start=1542, len=1),\n",
       "   Token(form='변화', tag='NNG', start=1544, len=2),\n",
       "   Token(form='를', tag='JKO', start=1546, len=1),\n",
       "   Token(form='맞이', tag='XR', start=1548, len=2),\n",
       "   Token(form='하', tag='XSV', start=1550, len=1),\n",
       "   Token(form='ᆯ', tag='ETM', start=1550, len=1),\n",
       "   Token(form='준비', tag='NNG', start=1552, len=2),\n",
       "   Token(form='를', tag='JKO', start=1554, len=1),\n",
       "   Token(form='하', tag='VV', start=1556, len=1),\n",
       "   Token(form='어야', tag='EC', start=1556, len=2),\n",
       "   Token(form='하', tag='VX', start=1558, len=1),\n",
       "   Token(form='ᆯ', tag='ETM', start=1558, len=1),\n",
       "   Token(form='것', tag='NNB', start=1560, len=1),\n",
       "   Token(form='이', tag='VCP', start=1561, len=1),\n",
       "   Token(form='다', tag='EF', start=1562, len=1),\n",
       "   Token(form='.', tag='SF', start=1563, len=1),\n",
       "   Token(form='모든', tag='MM', start=1565, len=2),\n",
       "   Token(form='것', tag='NNB', start=1568, len=1),\n",
       "   Token(form='에', tag='JKB', start=1569, len=1),\n",
       "   Token(form='장단점', tag='NNG', start=1571, len=3),\n",
       "   Token(form='이', tag='JKS', start=1574, len=1),\n",
       "   Token(form='있', tag='VV', start=1576, len=1),\n",
       "   Token(form='듯이', tag='EC', start=1577, len=2),\n",
       "   Token(form='새롭', tag='VA-I', start=1580, len=3),\n",
       "   Token(form='은', tag='ETM', start=1582, len=1),\n",
       "   Token(form='세상', tag='NNG', start=1584, len=2),\n",
       "   Token(form='의', tag='JKG', start=1586, len=1),\n",
       "   Token(form='장점', tag='NNG', start=1588, len=2),\n",
       "   Token(form='도', tag='JX', start=1590, len=1),\n",
       "   Token(form='분명히', tag='MAG', start=1592, len=3),\n",
       "   Token(form='있', tag='VV', start=1596, len=1),\n",
       "   Token(form='을', tag='ETM', start=1597, len=1),\n",
       "   Token(form='터', tag='NNB', start=1598, len=1),\n",
       "   Token(form='이', tag='VCP', start=1598, len=1),\n",
       "   Token(form='니', tag='EC', start=1599, len=1),\n",
       "   Token(form='조금', tag='NNG', start=1601, len=2),\n",
       "   Token(form='은', tag='JX', start=1603, len=1),\n",
       "   Token(form='덜', tag='MAG', start=1605, len=1),\n",
       "   Token(form='무서워하', tag='VV', start=1607, len=4),\n",
       "   Token(form='어도', tag='EC', start=1610, len=2),\n",
       "   Token(form='되', tag='VV', start=1613, len=1),\n",
       "   Token(form='지', tag='EC', start=1614, len=1),\n",
       "   Token(form='않', tag='VX', start=1616, len=1),\n",
       "   Token(form='을까', tag='EF', start=1617, len=2),\n",
       "   Token(form='?', tag='SF', start=1619, len=1)],\n",
       "  -3719.52197265625)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiwi = Kiwi()\n",
    "kiwi.analyze(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7873dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사 추출 함수\n",
    "def noun_extractor(text):\n",
    "    results = []\n",
    "    result = kiwi.analyze(text)\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if len(token) != 1 and pos.startswith('N') or pos.startswith('SL'):\n",
    "            results.append(token)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "448c82b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['저자',\n",
       " '과거',\n",
       " '현재',\n",
       " '변화',\n",
       " '변화',\n",
       " '한국',\n",
       " '사회',\n",
       " '데이터',\n",
       " '이야기',\n",
       " '결의',\n",
       " '현상',\n",
       " '나열',\n",
       " '저자',\n",
       " '통찰',\n",
       " '오랜만',\n",
       " '생각',\n",
       " '사람',\n",
       " '이야기',\n",
       " '이해',\n",
       " '전달',\n",
       " '능력',\n",
       " '사람',\n",
       " '변화',\n",
       " '사회',\n",
       " '사회',\n",
       " '구성원',\n",
       " '개개인',\n",
       " '자세',\n",
       " '제시',\n",
       " '세대',\n",
       " '관리자',\n",
       " '바람',\n",
       " '중간',\n",
       " '관리자',\n",
       " '무능',\n",
       " '고집',\n",
       " '지적',\n",
       " '부분',\n",
       " '사회생활',\n",
       " '시작',\n",
       " '가족',\n",
       " '친구',\n",
       " '대화',\n",
       " '책임',\n",
       " '상사',\n",
       " '얘기',\n",
       " '어른',\n",
       " '스스로',\n",
       " '경계',\n",
       " '중간중간',\n",
       " '문장',\n",
       " '당신',\n",
       " '메시지',\n",
       " '부제',\n",
       " '챕터',\n",
       " '인상',\n",
       " '요즘',\n",
       " '기록',\n",
       " '중요',\n",
       " '체감',\n",
       " '개인',\n",
       " '기억',\n",
       " '어디',\n",
       " '활동',\n",
       " '생각',\n",
       " '지금',\n",
       " '그거',\n",
       " '기록',\n",
       " '생각',\n",
       " '정리',\n",
       " '그거',\n",
       " '기억',\n",
       " '형태',\n",
       " '기록',\n",
       " '학벌',\n",
       " '성적',\n",
       " '증명',\n",
       " '요즘',\n",
       " '그거',\n",
       " '세상',\n",
       " '방향',\n",
       " '생각',\n",
       " '저자',\n",
       " '일상',\n",
       " '기록',\n",
       " '자신',\n",
       " '표현',\n",
       " '메시지',\n",
       " '이력서',\n",
       " '포트폴리오',\n",
       " '자신',\n",
       " '소개',\n",
       " '이제',\n",
       " '소비',\n",
       " 'sns',\n",
       " '관계',\n",
       " '표현',\n",
       " '시대',\n",
       " '사람',\n",
       " '필요',\n",
       " '구매',\n",
       " '물질',\n",
       " '소비',\n",
       " '자신',\n",
       " '개성',\n",
       " '의미',\n",
       " '표현',\n",
       " '의미',\n",
       " '소비',\n",
       " '요즘',\n",
       " '소비',\n",
       " '패턴',\n",
       " '물건',\n",
       " '브랜드',\n",
       " '추구',\n",
       " '가치',\n",
       " '지향',\n",
       " '경우',\n",
       " '마음',\n",
       " '소비',\n",
       " '해당',\n",
       " '브랜드',\n",
       " '기억',\n",
       " '콘텐츠',\n",
       " '소비',\n",
       " '마찬가지',\n",
       " '콘텐츠',\n",
       " '선택',\n",
       " '취향',\n",
       " '은연중',\n",
       " 'sns',\n",
       " '누구',\n",
       " '팔로우',\n",
       " '관심사',\n",
       " '네트워크',\n",
       " '공개',\n",
       " '행위',\n",
       " '일종',\n",
       " '저자',\n",
       " '결국',\n",
       " '수렴',\n",
       " '라이프',\n",
       " '스타일',\n",
       " '메시지',\n",
       " '한편',\n",
       " '기록',\n",
       " '수단',\n",
       " '비밀',\n",
       " '사회',\n",
       " '지금',\n",
       " '현실',\n",
       " '안주',\n",
       " '변화',\n",
       " '거부',\n",
       " '변화',\n",
       " '해당',\n",
       " '생각',\n",
       " '저자',\n",
       " '서문',\n",
       " '이야기',\n",
       " '변화',\n",
       " '변화',\n",
       " '준비',\n",
       " '장단점',\n",
       " '세상',\n",
       " '장점',\n",
       " '조금']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_nouns = noun_extractor(doc)\n",
    "tokenized_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "792e96b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'저자 과거 현재 변화 변화 한국 사회 데이터 이야기 결의 현상 나열 저자 통찰 오랜만 생각 사람 이야기 이해 전달 능력 사람 변화 사회 사회 구성원 개개인 자세 제시 세대 관리자 바람 중간 관리자 무능 고집 지적 부분 사회생활 시작 가족 친구 대화 책임 상사 얘기 어른 스스로 경계 중간중간 문장 당신 메시지 부제 챕터 인상 요즘 기록 중요 체감 개인 기억 어디 활동 생각 지금 그거 기록 생각 정리 그거 기억 형태 기록 학벌 성적 증명 요즘 그거 세상 방향 생각 저자 일상 기록 자신 표현 메시지 이력서 포트폴리오 자신 소개 이제 소비 sns 관계 표현 시대 사람 필요 구매 물질 소비 자신 개성 의미 표현 의미 소비 요즘 소비 패턴 물건 브랜드 추구 가치 지향 경우 마음 소비 해당 브랜드 기억 콘텐츠 소비 마찬가지 콘텐츠 선택 취향 은연중 sns 누구 팔로우 관심사 네트워크 공개 행위 일종 저자 결국 수렴 라이프 스타일 메시지 한편 기록 수단 비밀 사회 지금 현실 안주 변화 거부 변화 해당 생각 저자 서문 이야기 변화 변화 준비 장단점 세상 장점 조금'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(nouns)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "86d6280c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('저자', 0.5892),\n",
       " ('얘기', 0.5879),\n",
       " ('요즘', 0.5826),\n",
       " ('소개', 0.5788),\n",
       " ('이야기', 0.5707),\n",
       " ('수렴', 0.5695),\n",
       " ('세상', 0.5643),\n",
       " ('지금', 0.5524),\n",
       " ('팔로우', 0.5369),\n",
       " ('문장', 0.529),\n",
       " ('어디', 0.5223),\n",
       " ('필요', 0.5217),\n",
       " ('제시', 0.5212),\n",
       " ('안주', 0.5194),\n",
       " ('사람', 0.5178),\n",
       " ('결의', 0.5172),\n",
       " ('친구', 0.5117),\n",
       " ('이제', 0.5109),\n",
       " ('행위', 0.5068),\n",
       " ('그거', 0.5055)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=20)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2ec10243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('표현하는 메시지가', 0.47),\n",
       " ('현재까지 변화해온', 0.4718),\n",
       " ('책의 부제이기도', 0.4733),\n",
       " ('일상의 모든', 0.4757),\n",
       " ('단순히 현상만', 0.4822),\n",
       " ('전달하는 능력을', 0.4886),\n",
       " ('나의 관심사와', 0.4914),\n",
       " ('책은 변화하는', 0.5083),\n",
       " ('활동들과 생각을', 0.5415),\n",
       " ('중간관리자들의 무능과', 0.6187)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(doc,keyphrase_ngram_range=(1,2),use_maxsum = True,top_n = 10, highlight=False)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2340afbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db92a3f7ea3d437bae61c7520a5fde35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab504e00453144aa8d720581bf79eae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5c9a4123c045868433d338325d2577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da39ad19ff0f4f46aa1532d4e77bf357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16743cf8ac194e81a5d2c2d3235f681f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15bbc5cb5be4097b1bc1e226df46ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c9cc07738c44258324837b84e9932b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737d52c0480a46cda33633a2b3bf0994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8b4ad0d76d4ca9a8729f1fef9a4e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7695903dd44f39b157d4020e33e6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7add7e780be466d9b2d1106e94ad814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851b57d326644fa380b8bc3e4d43a21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef0a20d122f43dc9b31b0b8b17dfc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712130fb7cb24f8382faf292067d100b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('언젠가는 나에게도 변화를 맞이할', 0.5852),\n",
       " ('현상만 나열하는 아니라 저자의', 0.5858),\n",
       " ('구매하는 물질소비가 아닌 자신의', 0.5862),\n",
       " ('활동들과 생각을 보여줘야할 특히', 0.5867),\n",
       " ('하는 바람이다 중간관리자들의', 0.5868),\n",
       " ('언젠가는 나에게도 변화를', 0.5884),\n",
       " ('않았다 해도 변화는 분명히', 0.5911),\n",
       " ('바람이다 중간관리자들의', 0.5914),\n",
       " ('읽어봤으면 하는 바람이다 중간관리자들의', 0.5915),\n",
       " ('가족 친구들과 대화하다보면 일도', 0.5921),\n",
       " ('해도 변화는 분명히', 0.5933),\n",
       " ('있다 언젠가는 나에게도 변화를', 0.5942),\n",
       " ('바람이다 중간관리자들의 무능과 고집을', 0.6052),\n",
       " ('생각한다 저자는 일상의', 0.6072),\n",
       " ('해도 변화는 분명히 일어나고', 0.6094),\n",
       " ('것이라 생각한다 저자는 일상의', 0.6146),\n",
       " ('중간관리자들의 무능과', 0.6187),\n",
       " ('어딘가에 내가 활동들과 생각을', 0.6226),\n",
       " ('바람이다 중간관리자들의 무능과', 0.623),\n",
       " ('하는 바람이다 중간관리자들의 무능과', 0.6288)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc,keyphrase_ngram_range=(2,4),use_maxsum = True,top_n = 20)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "13970f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('바람이다 중간관리자들의 무능과', 0.623),\n",
       " ('생각한다 저자는 일상의', 0.6072),\n",
       " ('해도 변화는 분명히', 0.5933),\n",
       " ('언젠가는 나에게도 변화를', 0.5884),\n",
       " ('하는 바람이다 중간관리자들의', 0.5868),\n",
       " ('중간관리자들의 무능과 고집을', 0.5705),\n",
       " ('활동들과 생각을', 0.5415),\n",
       " ('좋았다 책은 변화하는', 0.5205),\n",
       " ('변화해온 앞으로 변화할', 0.5159),\n",
       " ('대화하다보면 일도 못하면서', 0.5129),\n",
       " ('팔로우하느냐도 나의 관심사와', 0.5055),\n",
       " ('나열하는 아니라 저자의', 0.5012),\n",
       " ('개성을 드러내고 의미를', 0.4814),\n",
       " ('포트폴리오로 자신을 소개하는', 0.4448),\n",
       " ('나의 관심사와 네트워크를', 0.4387),\n",
       " ('들어 기록의 중요성을', 0.4192),\n",
       " ('비슷한 결의 책을', 0.4021),\n",
       " ('있을테니 조금은 무서워해도', 0.3831),\n",
       " ('절실히 느낀다', 0.3396),\n",
       " ('같기도 하다', 0.2738)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_mmr = kw_model.extract_keywords(doc,keyphrase_ngram_range=(2,3),use_mmr = True,top_n = 20,diversity = 0.5)\n",
    "keywords_mmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fbb6ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#명사로 trigram 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ab19522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['저자',\n",
       " '과거',\n",
       " '현재',\n",
       " '변화',\n",
       " '변화',\n",
       " '한국',\n",
       " '사회',\n",
       " '데이터',\n",
       " '이야기',\n",
       " '결의',\n",
       " '현상',\n",
       " '나열',\n",
       " '저자',\n",
       " '통찰',\n",
       " '오랜만',\n",
       " '생각',\n",
       " '사람',\n",
       " '이야기',\n",
       " '이해',\n",
       " '전달',\n",
       " '능력',\n",
       " '사람',\n",
       " '변화',\n",
       " '사회',\n",
       " '사회',\n",
       " '구성원',\n",
       " '개개인',\n",
       " '자세',\n",
       " '제시',\n",
       " '세대',\n",
       " '관리자',\n",
       " '바람',\n",
       " '중간',\n",
       " '관리자',\n",
       " '무능',\n",
       " '고집',\n",
       " '지적',\n",
       " '부분',\n",
       " '사회생활',\n",
       " '시작',\n",
       " '가족',\n",
       " '친구',\n",
       " '대화',\n",
       " '책임',\n",
       " '상사',\n",
       " '얘기',\n",
       " '어른',\n",
       " '스스로',\n",
       " '경계',\n",
       " '중간중간',\n",
       " '문장',\n",
       " '당신',\n",
       " '메시지',\n",
       " '부제',\n",
       " '챕터',\n",
       " '인상',\n",
       " '요즘',\n",
       " '기록',\n",
       " '중요',\n",
       " '체감',\n",
       " '개인',\n",
       " '기억',\n",
       " '어디',\n",
       " '활동',\n",
       " '생각',\n",
       " '지금',\n",
       " '그거',\n",
       " '기록',\n",
       " '생각',\n",
       " '정리',\n",
       " '그거',\n",
       " '기억',\n",
       " '형태',\n",
       " '기록',\n",
       " '학벌',\n",
       " '성적',\n",
       " '증명',\n",
       " '요즘',\n",
       " '그거',\n",
       " '세상',\n",
       " '방향',\n",
       " '생각',\n",
       " '저자',\n",
       " '일상',\n",
       " '기록',\n",
       " '자신',\n",
       " '표현',\n",
       " '메시지',\n",
       " '이력서',\n",
       " '포트폴리오',\n",
       " '자신',\n",
       " '소개',\n",
       " '이제',\n",
       " '소비',\n",
       " 'sns',\n",
       " '관계',\n",
       " '표현',\n",
       " '시대',\n",
       " '사람',\n",
       " '필요',\n",
       " '구매',\n",
       " '물질',\n",
       " '소비',\n",
       " '자신',\n",
       " '개성',\n",
       " '의미',\n",
       " '표현',\n",
       " '의미',\n",
       " '소비',\n",
       " '요즘',\n",
       " '소비',\n",
       " '패턴',\n",
       " '물건',\n",
       " '브랜드',\n",
       " '추구',\n",
       " '가치',\n",
       " '지향',\n",
       " '경우',\n",
       " '마음',\n",
       " '소비',\n",
       " '해당',\n",
       " '브랜드',\n",
       " '기억',\n",
       " '콘텐츠',\n",
       " '소비',\n",
       " '마찬가지',\n",
       " '콘텐츠',\n",
       " '선택',\n",
       " '취향',\n",
       " '은연중',\n",
       " 'sns',\n",
       " '누구',\n",
       " '팔로우',\n",
       " '관심사',\n",
       " '네트워크',\n",
       " '공개',\n",
       " '행위',\n",
       " '일종',\n",
       " '저자',\n",
       " '결국',\n",
       " '수렴',\n",
       " '라이프',\n",
       " '스타일',\n",
       " '메시지',\n",
       " '한편',\n",
       " '기록',\n",
       " '수단',\n",
       " '비밀',\n",
       " '사회',\n",
       " '지금',\n",
       " '현실',\n",
       " '안주',\n",
       " '변화',\n",
       " '거부',\n",
       " '변화',\n",
       " '해당',\n",
       " '생각',\n",
       " '저자',\n",
       " '서문',\n",
       " '이야기',\n",
       " '변화',\n",
       " '변화',\n",
       " '준비',\n",
       " '장단점',\n",
       " '세상',\n",
       " '장점',\n",
       " '조금']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "67f6d6ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m n_gram_range \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[43mCountVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_gram_range\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenized_nouns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m candidates \u001b[38;5;241m=\u001b[39m count\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrigram 개수 :\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mlen\u001b[39m(candidates))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1334\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m \n\u001b[0;32m   1321\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1334\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "n_gram_range = (2, 3)\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 :',len(candidates))\n",
    "print('trigram 다섯개만 출력 :',candidates[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ba800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c8a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3fbc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer and sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5607cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\liber\\anaconda3\\lib\\site-packages (4.19.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\liber\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\liber\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\liber\\anaconda3\\lib\\site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1d3039f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (992) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 992].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# BERT 모델로부터 확률 분포 얻기\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 32\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# [MASK] 위치의 예측 확률 분포에서 상위 N 단어 추출\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1358\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1358\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1373\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:988\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    987\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[1;32m--> 988\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (992) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 992].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from collections import Counter\n",
    "\n",
    "# 한국어 BERT 모델과 토크나이저 로드 (여기서는 'bert-base-multilingual-cased' 사용)\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "# model_name = 'skt/kobert-base-v1'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# 입력 문서\n",
    "# document = \"한국어 문서에서 키워드 추출 예제입니다.\"\n",
    "document = doc\n",
    "\n",
    "# 문장을 토큰화하고 [MASK] 토큰 삽입\n",
    "tokens = tokenizer.tokenize(document)\n",
    "masked_index = tokens.index(\"[MASK]\") if \"[MASK]\" in tokens else -1\n",
    "\n",
    "if masked_index == -1:\n",
    "    # 문장에 [MASK] 토큰이 없는 경우, 끝에 추가\n",
    "    tokens += [\"[MASK]\"]\n",
    "else:\n",
    "    # [MASK] 토큰이 이미 있는 경우, 제거하고 끝에 추가\n",
    "    tokens[masked_index] = \"[MASK]\"\n",
    "\n",
    "# BERT 모델 입력 형식으로 변환\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_tensor = torch.tensor(input_ids).unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "# BERT 모델로부터 확률 분포 얻기\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# [MASK] 위치의 예측 확률 분포에서 상위 N 단어 추출\n",
    "N = 5\n",
    "top_k_tokens = torch.topk(predictions[0, masked_index], N).indices.tolist()\n",
    "\n",
    "# 상위 N 단어를 토큰으로 변환하여 출력\n",
    "keywords = [tokenizer.decode([token]) for token in top_k_tokens]\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "071ffb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     tokens[masked_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# BERT 모델 입력 형식으로 변환\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_ids)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# 배치 차원 추가\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 배치 크기 조절 (예: 32개의 문장을 동시에 처리하려면 여기서 배치 크기를 32로 설정)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py:586\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    584\u001b[0m ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m--> 586\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id_with_added_voc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py:593\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madded_tokens_encoder\u001b[49m:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder[token]\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_token_to_id(token)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from collections import Counter\n",
    "\n",
    "# 한국어 BERT 모델과 토크나이저 로드 (여기서는 'bert-base-multilingual-cased' 사용)\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "model_name = 'skt/kobert-base-v1'\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#model = BertForMaskedLM.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "# 입력 문서\n",
    "# document = \"한국어 문서에서 키워드 추출 예제입니다.\"\n",
    "document = doc\n",
    "# 시퀀스 길이와 배치 크기 조절\n",
    "max_seq_length = 128  # 최대 시퀀스 길이\n",
    "batch_size = 32  # 배치 크기\n",
    "\n",
    "# 문장을 토큰화하고 [MASK] 토큰 삽입\n",
    "tokens = kiwi.analyze(doc) #kiwi로 토크나이즈\n",
    "#tokens = tokenizer.tokenize(document)[:max_seq_length]\n",
    "masked_index = tokens.index(\"[MASK]\") if \"[MASK]\" in tokens else -1\n",
    "\n",
    "if masked_index == -1:\n",
    "    # 문장에 [MASK] 토큰이 없는 경우, 끝에 추가\n",
    "    tokens += [\"[MASK]\"]\n",
    "else:\n",
    "    # [MASK] 토큰이 이미 있는 경우, 제거하고 끝에 추가\n",
    "    tokens[masked_index] = \"[MASK]\"\n",
    "\n",
    "# BERT 모델 입력 형식으로 변환\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_tensor = torch.tensor(input_ids).unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "# 배치 크기 조절 (예: 32개의 문장을 동시에 처리하려면 여기서 배치 크기를 32로 설정)\n",
    "input_tensor = input_tensor.repeat(batch_size, 1)\n",
    "\n",
    "# BERT 모델로부터 확률 분포 얻기\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# [MASK] 위치의 예측 확률 분포에서 상위 N 단어 추출\n",
    "N = 5\n",
    "top_k_tokens = torch.topk(predictions[:, masked_index], N).indices.tolist()\n",
    "\n",
    "# 상위 N 단어를 토큰으로 변환하여 출력\n",
    "keywords = [tokenizer.decode([token]) for token in top_k_tokens[0]]\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 추출 함수\n",
    "def kiwi_token_extractor(text):\n",
    "    results = []\n",
    "    result = kiwi.analyze(text)\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if len(token) != 1 and pos.startswith('N') or pos.startswith('SL'):\n",
    "            results.append(token)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b87cdf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (920) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [32, 920].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 36\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1358\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1358\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1373\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:988\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    987\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[1;32m--> 988\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (920) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [32, 920].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "kiwi = Kiwi()\n",
    "#kiwi.prepare()\n",
    "\n",
    "# sentence = \"한국어 문장을 형태소 분석하기\"\n",
    "document = doc\n",
    "# 시퀀스 길이와 배치 크기 조절\n",
    "max_seq_length = 128  # 최대 시퀀스 길이\n",
    "batch_size = 32  # 배치 크기\n",
    "\n",
    "result = kiwi.analyze(document)\n",
    "\n",
    "#token 뽑기\n",
    "tokens = [token for token in result[0][0]]\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "#input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "#input_tensor = torch.tensor(input_ids).unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "# BERT 모델 입력 형식으로 변환\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_tensor = torch.tensor(input_ids).unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "# 배치 크기 조절 (예: 32개의 문장을 동시에 처리하려면 여기서 배치 크기를 32로 설정)\n",
    "input_tensor = input_tensor.repeat(batch_size, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# 키워드 추출 또는 다른 작업 수행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4cbbc25e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     tokens[masked_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# BERT 모델 입력 형식으로 변환\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_ids)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# 배치 차원 추가\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 배치 크기 조절 (예: 32개의 문장을 동시에 처리하려면 여기서 배치 크기를 32로 설정)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py:586\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    584\u001b[0m ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m--> 586\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id_with_added_voc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py:593\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madded_tokens_encoder\u001b[49m:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder[token]\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_token_to_id(token)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from collections import Counter\n",
    "\n",
    "# 한국어 BERT 모델과 토크나이저 로드 (여기서는 'bert-base-multilingual-cased' 사용)\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "model_name = 'skt/kobert-base-v1'\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#model = BertForMaskedLM.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "kiwi = Kiwi()\n",
    "#kiwi.prepare()\n",
    "\n",
    "# 입력 문서\n",
    "# document = \"한국어 문서에서 키워드 추출 예제입니다.\"\n",
    "document = doc\n",
    "# 시퀀스 길이와 배치 크기 조절\n",
    "max_seq_length = 128  # 최대 시퀀스 길이\n",
    "batch_size = 32  # 배치 크기\n",
    "\n",
    "result = kiwi.analyze(document)\n",
    "\n",
    "#token 뽑기\n",
    "tokens = [token for token in result[0][0]]\n",
    "\n",
    "\n",
    "# 문장을 토큰화하고 [MASK] 토큰 삽입\n",
    "tokens = kiwi.analyze(doc) #kiwi로 토크나이즈\n",
    "#tokens = tokenizer.tokenize(document)[:max_seq_length]\n",
    "masked_index = tokens.index(\"[MASK]\") if \"[MASK]\" in tokens else -1\n",
    "\n",
    "if masked_index == -1:\n",
    "    # 문장에 [MASK] 토큰이 없는 경우, 끝에 추가\n",
    "    tokens += [\"[MASK]\"]\n",
    "else:\n",
    "    # [MASK] 토큰이 이미 있는 경우, 제거하고 끝에 추가\n",
    "    tokens[masked_index] = \"[MASK]\"\n",
    "\n",
    "# BERT 모델 입력 형식으로 변환\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_tensor = torch.tensor(input_ids).unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "# 배치 크기 조절 (예: 32개의 문장을 동시에 처리하려면 여기서 배치 크기를 32로 설정)\n",
    "input_tensor = input_tensor.repeat(batch_size, 1)\n",
    "\n",
    "# BERT 모델로부터 확률 분포 얻기\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# [MASK] 위치의 예측 확률 분포에서 상위 N 단어 추출\n",
    "N = 5\n",
    "top_k_tokens = torch.topk(predictions[:, masked_index], N).indices.tolist()\n",
    "\n",
    "# 상위 N 단어를 토큰으로 변환하여 출력\n",
    "keywords = [tokenizer.decode([token]) for token in top_k_tokens[0]]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8cc95158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 한국어 BERT 모델과 토크나이저 로드 (여기서는 'bert-base-multilingual-cased' 사용)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# model_name = \"bert-base-multilingual-cased\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskt/kobert-base-v1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 입력 문서\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1854\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1852\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2017\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   2016\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2017\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2019\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2022\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:213\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[1;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     vocab_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    198\u001b[0m ):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    200\u001b[0m         do_lower_case\u001b[38;5;241m=\u001b[39mdo_lower_case,\n\u001b[0;32m    201\u001b[0m         do_basic_tokenize\u001b[38;5;241m=\u001b[39mdo_basic_tokenize,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a vocabulary file at path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. To load the vocabulary from a Google pretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         )\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m load_vocab(vocab_file)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# Kiwi 형태소 분석기 초기화\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 한국어 BERT 모델과 토크나이저 로드 (여기서는 'bert-base-multilingual-cased' 사용)\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "model_name = 'skt/kobert-base-v1'\n",
    "#tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10)\n",
    "keywords\n",
    "\n",
    "# 입력 문서\n",
    "document = \"한국어 문서에서 키워드 추출 예제입니다.\"\n",
    "\n",
    "# Kiwi로 문장을 형태소 분석하고 토큰화\n",
    "result = kiwi.analyze(document)\n",
    "#tokens = [morph.lex for word in result[0][0]]\n",
    "tokens = [token for token in result[0][0]]\n",
    "\n",
    "# BERT 모델 입력 형식으로 변환\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_tensor = torch.tensor(input_ids).unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "# 텐서 크기 조절 (예: 시퀀스 길이를 128로, 배치 크기를 1로 설정)\n",
    "max_seq_length = 128\n",
    "batch_size = 1\n",
    "input_tensor = input_tensor[:, :max_seq_length]  # 시퀀스 길이 제한\n",
    "input_tensor = input_tensor.repeat(batch_size, 1)  # 배치 크기 조절\n",
    "\n",
    "# BERT 모델로부터 확률 분포 얻기\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# [MASK] 위치의 예측 확률 분포에서 상위 N 단어 추출\n",
    "masked_index = tokens.index(\"[MASK]\") if \"[MASK]\" in tokens else -1\n",
    "N = 5\n",
    "top_k_tokens = torch.topk(predictions[:, masked_index], N).indices.tolist()\n",
    "\n",
    "# 상위 N 단어를 토큰으로 변환하여 출력\n",
    "keywords = [tokenizer.decode([token]) for token in top_k_tokens[0]]\n",
    "print(keywords)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
